{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 工具导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import gc\n",
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    " \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据集\n",
    "\n",
    "#test_data = pd.read_csv('./data_format1/test_format1.csv')\n",
    "#train_data = pd.read_csv('./data_format1/train_format1.csv')\n",
    "\n",
    "#user_info = pd.read_csv('./data_format1/user_info_format1.csv')\n",
    "#user_log = pd.read_csv('./data_format1/user_log_format1.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据资源查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_log.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_name, num_rows):\n",
    "    return pd.read_csv(file_name, nrows=num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 内存压缩方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce memory\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据进行内存压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 1.74 MB\n",
      "Decreased by 70.8%\n",
      "Memory usage after optimization is: 3.49 MB\n",
      "Decreased by 41.7%\n",
      "Memory usage after optimization is: 3.24 MB\n",
      "Decreased by 66.7%\n",
      "Memory usage after optimization is: 890.47 MB\n",
      "Decreased by 69.6%\n"
     ]
    }
   ],
   "source": [
    "num_rows = None\n",
    "#num_rows = 2000  # 1000条测试代码使用\n",
    "\n",
    "train_file = './data_format1/train_format1.csv'\n",
    "test_file = './data_format1/test_format1.csv'\n",
    "\n",
    "user_info_file = './data_format1/user_info_format1.csv'\n",
    "user_log_file = './data_format1/user_log_format1.csv'\n",
    "\n",
    "train_data = reduce_mem_usage(read_csv(train_file, num_rows))\n",
    "test_data = reduce_mem_usage(read_csv(test_file, num_rows))\n",
    "\n",
    "user_info = reduce_mem_usage(read_csv(user_info_file, num_rows))\n",
    "user_log = reduce_mem_usage(read_csv(user_log_file, num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260864 entries, 0 to 260863\n",
      "Data columns (total 3 columns):\n",
      "user_id        260864 non-null int32\n",
      "merchant_id    260864 non-null int16\n",
      "label          260864 non-null int8\n",
      "dtypes: int16(1), int32(1), int8(1)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 261477 entries, 0 to 261476\n",
      "Data columns (total 3 columns):\n",
      "user_id        261477 non-null int32\n",
      "merchant_id    261477 non-null int16\n",
      "prob           0 non-null float64\n",
      "dtypes: float64(1), int16(1), int32(1)\n",
      "memory usage: 3.5 MB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 424170 entries, 0 to 424169\n",
      "Data columns (total 3 columns):\n",
      "user_id      424170 non-null int32\n",
      "age_range    421953 non-null float16\n",
      "gender       417734 non-null float16\n",
      "dtypes: float16(2), int32(1)\n",
      "memory usage: 3.2 MB\n"
     ]
    }
   ],
   "source": [
    "user_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54925330 entries, 0 to 54925329\n",
      "Data columns (total 7 columns):\n",
      "user_id        int32\n",
      "item_id        int32\n",
      "cat_id         int16\n",
      "seller_id      int16\n",
      "brand_id       float16\n",
      "time_stamp     int16\n",
      "action_type    int8\n",
      "dtypes: float16(1), int16(3), int32(2), int8(1)\n",
      "memory usage: 890.5 MB\n"
     ]
    }
   ],
   "source": [
    "user_log.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "\n",
    "### 合并用户信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = train_data.append(test_data)\n",
    "all_data = all_data.merge(user_info,on=['user_id'],how='left')\n",
    "del train_data, test_data, user_info\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用户行为日志信息按时间进行排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "按时间排序\n",
    "\"\"\"\n",
    "user_log = user_log.sort_values(['user_id','time_stamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对每个用户的逐个合并所有的item_id, cat_id,seller_id,brand_id,time_stamp, action_type字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "合并数据\n",
    "\"\"\"\n",
    "list_join_func = lambda x: \" \".join([str(i) for i in x])\n",
    "\n",
    "\n",
    "agg_dict = {\n",
    "            'item_id' : list_join_func,\t\n",
    "            'cat_id' : list_join_func,\n",
    "            'seller_id' : list_join_func,\n",
    "            'brand_id' : list_join_func,\n",
    "            'time_stamp' : list_join_func,\n",
    "            'action_type' : list_join_func\n",
    "        }\n",
    "\n",
    "rename_dict = {\n",
    "            'item_id' : 'item_path',\n",
    "            'cat_id' : 'cat_path',\n",
    "            'seller_id' : 'seller_path',\n",
    "            'brand_id' : 'brand_path',\n",
    "            'time_stamp' : 'time_stamp_path',\n",
    "            'action_type' : 'action_type_path'\n",
    "        }\n",
    "\n",
    "def merge_list(df_ID, join_columns, df_data, agg_dict, rename_dict):\n",
    "    df_data = df_data.\\\n",
    "            groupby(join_columns).\\\n",
    "            agg(agg_dict).\\\n",
    "            reset_index().\\\n",
    "            rename(columns=rename_dict)\n",
    "\n",
    "    df_ID = df_ID.merge(df_data, on=join_columns, how=\"left\")\n",
    "    return df_ID\n",
    "\n",
    "all_data = merge_list(all_data, 'user_id', user_log, agg_dict, rename_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除数据并回收内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "删除不需要的数据\n",
    "\"\"\"\n",
    "del user_log\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义数据统计函数\n",
    "\n",
    "### 统计数据的总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnt_(x):\n",
    "    try:\n",
    "        return len(x.split(' '))\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计唯一数据总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nunique_(x):\n",
    "    try:\n",
    "        return len(set(x.split(' ')))\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计数据最大值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_(x):\n",
    "    try:\n",
    "        return np.max([int(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计数据最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_(x):\n",
    "    try:\n",
    "        return np.min([int(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计数据的标准差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_(x):\n",
    "    try:\n",
    "        return np.std([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计数据中top N的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_n(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][0]\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计数据中top N数据的总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_n_cnt(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][1]\n",
    "    except:\n",
    "        return -1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "def user_cnt(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(cnt_)\n",
    "    return df_data\n",
    "\n",
    "def user_nunique(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(nunique_)\n",
    "    return df_data\n",
    "    \n",
    "def user_max(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(max_)\n",
    "    return df_data\n",
    "\n",
    "def user_min(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(min_)\n",
    "    return df_data\n",
    "    \n",
    "def user_std(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(std_)\n",
    "    return df_data\n",
    "\n",
    "def user_most_n(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data\n",
    "\n",
    "def user_most_n_cnt(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n_cnt(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取商铺的基本统计特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\t提取基本统计特征\n",
    "\"\"\"\n",
    "all_data_test = all_data.head(2000)\n",
    "#all_data_test = all_data\n",
    "# 统计用户 点击、浏览、加购、购买行为\n",
    "# 总次数\n",
    "all_data_test = user_cnt(all_data_test,  'seller_path', 'user_cnt')\n",
    "# 不同店铺个数\n",
    "all_data_test = user_nunique(all_data_test,  'seller_path', 'seller_nunique')\n",
    "# 不同品类个数\n",
    "all_data_test = user_nunique(all_data_test,  'cat_path', 'cat_nunique')\n",
    "# 不同品牌个数\n",
    "all_data_test = user_nunique(all_data_test,  'brand_path', 'brand_nunique')\n",
    "# 不同商品个数\n",
    "all_data_test = user_nunique(all_data_test,  'item_path', 'item_nunique')\n",
    "# 活跃天数\n",
    "all_data_test = user_nunique(all_data_test,  'time_stamp_path', 'time_stamp_nunique')\n",
    "# 不用行为种数\n",
    "all_data_test = user_nunique(all_data_test,  'action_type_path', 'action_type_nunique')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最晚时间\n",
    "all_data_test = user_max(all_data_test,  'time_stamp_path', 'time_stamp_max')\n",
    "# 最早时间\n",
    "all_data_test = user_min(all_data_test,  'time_stamp_path', 'time_stamp_min')\n",
    "# 活跃天数方差\n",
    "all_data_test = user_std(all_data_test,  'time_stamp_path', 'time_stamp_std')\n",
    "# 最早和最晚时间差\n",
    "all_data_test['time_stamp_range'] = all_data_test['time_stamp_max'] - all_data_test['time_stamp_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户最喜欢的店铺\n",
    "all_data_test = user_most_n(all_data_test, 'seller_path', 'seller_most_1', n=1)\n",
    "# 最喜欢的类目\n",
    "all_data_test = user_most_n(all_data_test, 'cat_path', 'cat_most_1', n=1)\n",
    "# 最喜欢的品牌\n",
    "all_data_test = user_most_n(all_data_test, 'brand_path', 'brand_most_1', n=1)\n",
    "# 最常见的行为动作\n",
    "all_data_test = user_most_n(all_data_test, 'action_type_path', 'action_type_1', n=1)\n",
    "# ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用户最喜欢的店铺 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'seller_path', 'seller_most_1_cnt', n=1)\n",
    "# 最喜欢的类目 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'cat_path', 'cat_most_1_cnt', n=1)\n",
    "# 最喜欢的品牌 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'brand_path', 'brand_most_1_cnt', n=1)\n",
    "# 最常见的行为动作 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'action_type_path', 'action_type_1_cnt', n=1)\n",
    "# ....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分开统计用户的点击，加购，购买，收藏特征\n",
    "\n",
    "### 不同行为的业务函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 点击、加购、购买、收藏 分开统计\n",
    "\"\"\"\n",
    "统计基本特征函数  \n",
    "-- 知识点二\n",
    "-- 根据不同行为的业务函数\n",
    "-- 提取不同特征\n",
    "\"\"\"\n",
    "def col_cnt_(df_data, columns_list, action_type):\n",
    "\ttry:\n",
    "\t    data_dict = {}\n",
    "\t    \n",
    "\t    col_list = copy.deepcopy(columns_list)\n",
    "\t    if action_type != None:\n",
    "\t        col_list += ['action_type_path']\n",
    "\n",
    "\t    for col in col_list:\n",
    "\t        data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "\t    path_len = len(data_dict[col])\n",
    "\t    \n",
    "\t    data_out = []\n",
    "\t    for i_ in range(path_len):\n",
    "\t        data_txt = ''\n",
    "\t        for col_ in columns_list:\n",
    "\t            if data_dict['action_type_path'][i_] == action_type:\n",
    "\t                data_txt += '_' + data_dict[col_][i_]\n",
    "\t        data_out.append(data_txt)\n",
    "\t    \n",
    "\t    return len(data_out)  \n",
    "\texcept:\n",
    "\t\treturn -1\n",
    "\n",
    "def col_nuique_(df_data, columns_list, action_type):\n",
    "\ttry:\n",
    "\t    data_dict = {}\n",
    "\t    \n",
    "\t    col_list = copy.deepcopy(columns_list)\n",
    "\t    if action_type != None:\n",
    "\t        col_list += ['action_type_path']\n",
    "\n",
    "\t    for col in col_list:\n",
    "\t        data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "\t    path_len = len(data_dict[col])\n",
    "\t    \n",
    "\t    data_out = []\n",
    "\t    for i_ in range(path_len):\n",
    "\t        data_txt = ''\n",
    "\t        for col_ in columns_list:\n",
    "\t            if data_dict['action_type_path'][i_] == action_type:\n",
    "\t                data_txt += '_' + data_dict[col_][i_]\n",
    "\t        data_out.append(data_txt)\n",
    "\n",
    "\t    return len(set(data_out))\n",
    "\texcept:\n",
    "\t\treturn -1\n",
    "    \n",
    "\n",
    "def user_col_cnt(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_cnt_(x, columns_list, action_type), axis=1)\n",
    "    return df_data\n",
    "\n",
    "def user_col_nunique(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_nuique_(x, columns_list, action_type), axis=1)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计店铺被用户点击次数，加购次数，购买次数，收藏次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 点击次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '0', 'user_cnt_0')\n",
    "# 加购次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '1', 'user_cnt_1')\n",
    "# 购买次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '2', 'user_cnt_2')\n",
    "# 收藏次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '3', 'user_cnt_3')\n",
    "\n",
    "\n",
    "# 不同店铺个数\n",
    "all_data_test = user_col_nunique(all_data_test,  ['seller_path'], '0', 'seller_nunique_0')\n",
    "# ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 组合特征\n",
    "\n",
    "### 特征组合进行业务特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 点击次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path', 'item_path'], '0', 'user_cnt_0')\n",
    "\n",
    "# 不同店铺个数\n",
    "all_data_test = user_col_nunique(all_data_test,  ['seller_path', 'item_path'], '0', 'seller_nunique_0')\n",
    "# ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看提取的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'merchant_id', 'prob', 'user_id', 'age_range', 'gender',\n",
       "       'item_path', 'cat_path', 'seller_path', 'brand_path', 'time_stamp_path',\n",
       "       'action_type_path', 'user_cnt', 'seller_nunique', 'cat_nunique',\n",
       "       'brand_nunique', 'item_nunique', 'time_stamp_nunique',\n",
       "       'action_type_nunique', 'time_stamp_max', 'time_stamp_min',\n",
       "       'time_stamp_std', 'time_stamp_range', 'seller_most_1', 'cat_most_1',\n",
       "       'brand_most_1', 'action_type_1', 'seller_most_1_cnt', 'cat_most_1_cnt',\n",
       "       'brand_most_1_cnt', 'action_type_1_cnt', 'user_cnt_0', 'user_cnt_1',\n",
       "       'user_cnt_2', 'user_cnt_3', 'seller_nunique_0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用countvector，tfidf提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 知识点四\n",
    "-- 利用countvector，tfidf提取特征\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from scipy import sparse\n",
    "# cntVec = CountVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 1), max_features=100)\n",
    "tfidfVec = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 1), max_features=100)\n",
    "\n",
    "\n",
    "# columns_list = ['seller_path', 'cat_path', 'brand_path', 'action_type_path', 'item_path', 'time_stamp_path']\n",
    "columns_list = ['seller_path']\n",
    "for i, col in enumerate(columns_list):\n",
    "\ttfidfVec.fit(all_data_test[col])\n",
    "\tdata_ = tfidfVec.transform(all_data_test[col])\n",
    "\tif i == 0:\n",
    "\t\tdata_cat = data_\n",
    "\telse:\n",
    "\t\tdata_cat = sparse.hstack((data_cat, data_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征重命名 特征合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = pd.DataFrame(data_cat.toarray())\n",
    "df_tfidf.columns = ['tfidf_' + str(i) for i in df_tfidf.columns]\n",
    "all_data_test = pd.concat([all_data_test, df_tfidf],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embeeding特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Train Word2Vec model\n",
    "\n",
    "model = gensim.models.Word2Vec(all_data_test['seller_path'].apply(lambda x: x.split(' ')), size=100, window=5, min_count=5, workers=4)\n",
    "# model.save(\"product2vec.model\")\n",
    "# model = gensim.models.Word2Vec.load(\"product2vec.model\")\n",
    "\n",
    "def mean_w2v_(x, model, size=100):\n",
    "    try:\n",
    "        i = 0\n",
    "        for word in x.split(' '):\n",
    "            if word in model.wv.vocab:\n",
    "                i += 1\n",
    "                if i == 1:\n",
    "                    vec = np.zeros(size)\n",
    "                vec += model.wv[word]\n",
    "        return vec / i \n",
    "    except:\n",
    "        return  np.zeros(size)\n",
    "\n",
    "\n",
    "def get_mean_w2v(df_data, columns, model, size):\n",
    "    data_array = []\n",
    "    for index, row in df_data.iterrows():\n",
    "        w2v = mean_w2v_(row[columns], model, size)\n",
    "        data_array.append(w2v)\n",
    "    return pd.DataFrame(data_array)\n",
    "\n",
    "df_embeeding = get_mean_w2v(all_data_test, 'seller_path', model, 100)\n",
    "df_embeeding.columns = ['embeeding_' + str(i) for i in df_embeeding.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embeeding特征和原始特征合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_test = pd.concat([all_data_test, df_embeeding],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stacking特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenxu/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-- 知识点六\n",
    "-- stacking特征\n",
    "\"\"\"\n",
    "from sklearn.cross_validation import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor,ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss,mean_absolute_error,mean_squared_error\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stacking 回归特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 回归\n",
    "-- stacking 回归特征\n",
    "\"\"\"\n",
    "def stacking_reg(clf,train_x,train_y,test_x,clf_name,kf,label_split=None):\n",
    "    train=np.zeros((train_x.shape[0],1))\n",
    "    test=np.zeros((test_x.shape[0],1))\n",
    "    test_pre=np.empty((folds,test_x.shape[0],1))\n",
    "    cv_scores=[]\n",
    "    for i,(train_index,test_index) in enumerate(kf.split(train_x,label_split)):       \n",
    "        tr_x=train_x[train_index]\n",
    "        tr_y=train_y[train_index]\n",
    "        te_x=train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "        if clf_name in [\"rf\",\"ada\",\"gb\",\"et\",\"lr\"]:\n",
    "            clf.fit(tr_x,tr_y)\n",
    "            pre=clf.predict(te_x).reshape(-1,1)\n",
    "            train[test_index]=pre\n",
    "            test_pre[i,:]=clf.predict(test_x).reshape(-1,1)\n",
    "            cv_scores.append(mean_squared_error(te_y, pre))\n",
    "        elif clf_name in [\"xgb\"]:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)\n",
    "            z = clf.DMatrix(test_x, label=te_y, missing=-1)\n",
    "            params = {'booster': 'gbtree',\n",
    "                      'eval_metric': 'rmse',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 5,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      'nthread': 12\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'),\n",
    "                         (test_matrix, 'eval')\n",
    "                         ]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix, num_boost_round=num_round,evals=watchlist,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(test_matrix,ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "                train[test_index]=pre\n",
    "                test_pre[i, :]= model.predict(z, ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "                cv_scores.append(mean_squared_error(te_y, pre))\n",
    "\n",
    "        elif clf_name in [\"lgb\"]:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            params = {\n",
    "                      'boosting_type': 'gbdt',\n",
    "                      'objective': 'regression_l2',\n",
    "                      'metric': 'mse',\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'num_leaves': 2**5,\n",
    "                      'lambda_l2': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'learning_rate': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      'nthread': 12,\n",
    "                      'silent': True,\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix,num_round,valid_sets=test_matrix,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(te_x,num_iteration=model.best_iteration).reshape(-1,1)\n",
    "                train[test_index]=pre\n",
    "                test_pre[i, :]= model.predict(test_x, num_iteration=model.best_iteration).reshape(-1,1)\n",
    "                cv_scores.append(mean_squared_error(te_y, pre))\n",
    "        else:\n",
    "            raise IOError(\"Please add new clf.\")\n",
    "        print(\"%s now score is:\"%clf_name,cv_scores)\n",
    "    test[:]=test_pre.mean(axis=0)\n",
    "    print(\"%s_score_list:\"%clf_name,cv_scores)\n",
    "    print(\"%s_score_mean:\"%clf_name,np.mean(cv_scores))\n",
    "    return train.reshape(-1,1),test.reshape(-1,1)\n",
    "\n",
    "def rf_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    randomforest = RandomForestRegressor(n_estimators=600, max_depth=20, n_jobs=-1, random_state=2017, max_features=\"auto\",verbose=1)\n",
    "    rf_train, rf_test = stacking_reg(randomforest, x_train, y_train, x_valid, \"rf\", kf, label_split=label_split)\n",
    "    return rf_train, rf_test,\"rf_reg\"\n",
    "\n",
    "def ada_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    adaboost = AdaBoostRegressor(n_estimators=30, random_state=2017, learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking_reg(adaboost, x_train, y_train, x_valid, \"ada\", kf, label_split=label_split)\n",
    "    return ada_train, ada_test,\"ada_reg\"\n",
    "\n",
    "def gb_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gbdt = GradientBoostingRegressor(learning_rate=0.04, n_estimators=100, subsample=0.8, random_state=2017,max_depth=5,verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking_reg(gbdt, x_train, y_train, x_valid, \"gb\", kf, label_split=label_split)\n",
    "    return gbdt_train, gbdt_test,\"gb_reg\"\n",
    "\n",
    "def et_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    extratree = ExtraTreesRegressor(n_estimators=600, max_depth=35, max_features=\"auto\", n_jobs=-1, random_state=2017,verbose=1)\n",
    "    et_train, et_test = stacking_reg(extratree, x_train, y_train, x_valid, \"et\", kf, label_split=label_split)\n",
    "    return et_train, et_test,\"et_reg\"\n",
    "\n",
    "def lr_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    lr_reg=LinearRegression(n_jobs=-1)\n",
    "    lr_train, lr_test = stacking_reg(lr_reg, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return lr_train, lr_test, \"lr_reg\"\n",
    "\n",
    "def xgb_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_reg(xgboost, x_train, y_train, x_valid, \"xgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"xgb_reg\"\n",
    "\n",
    "def lgb_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    lgb_train, lgb_test = stacking_reg(lightgbm, x_train, y_train, x_valid, \"lgb\", kf, label_split=label_split)\n",
    "    return lgb_train, lgb_test,\"lgb_reg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stacking 分类特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 分类\n",
    "-- stacking 分类特征\n",
    "\"\"\"\n",
    "def stacking_clf(clf,train_x,train_y,test_x,clf_name,kf,label_split=None):\n",
    "    train=np.zeros((train_x.shape[0],1))\n",
    "    test=np.zeros((test_x.shape[0],1))\n",
    "    test_pre=np.empty((folds,test_x.shape[0],1))\n",
    "    cv_scores=[]\n",
    "    for i,(train_index,test_index) in enumerate(kf.split(train_x,label_split)):       \n",
    "        tr_x=train_x[train_index]\n",
    "        tr_y=train_y[train_index]\n",
    "        te_x=train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "\n",
    "        if clf_name in [\"rf\",\"ada\",\"gb\",\"et\",\"lr\",\"knn\",\"gnb\"]:\n",
    "            clf.fit(tr_x,tr_y)\n",
    "            pre=clf.predict_proba(te_x)\n",
    "            \n",
    "            train[test_index]=pre[:,0].reshape(-1,1)\n",
    "            test_pre[i,:]=clf.predict_proba(test_x)[:,0].reshape(-1,1)\n",
    "            \n",
    "            cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        elif clf_name in [\"xgb\"]:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)\n",
    "            z = clf.DMatrix(test_x, label=te_y, missing=-1)\n",
    "            params = {'booster': 'gbtree',\n",
    "                      'objective': 'multi:softprob',\n",
    "                      'eval_metric': 'mlogloss',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 5,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      \"num_class\": 2\n",
    "                      }\n",
    "\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'),\n",
    "                         (test_matrix, 'eval')\n",
    "                         ]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix, num_boost_round=num_round,evals=watchlist,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(test_matrix,ntree_limit=model.best_ntree_limit)\n",
    "                train[test_index]=pre[:,0].reshape(-1,1)\n",
    "                test_pre[i, :]= model.predict(z, ntree_limit=model.best_ntree_limit)[:,0].reshape(-1,1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        elif clf_name in [\"lgb\"]:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            params = {\n",
    "                      'boosting_type': 'gbdt',\n",
    "                      #'boosting_type': 'dart',\n",
    "                      'objective': 'multiclass',\n",
    "                      'metric': 'multi_logloss',\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'num_leaves': 2**5,\n",
    "                      'lambda_l2': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'learning_rate': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      \"num_class\": 2,\n",
    "                      'silent': True,\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix,num_round,valid_sets=test_matrix,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(te_x,num_iteration=model.best_iteration)\n",
    "                train[test_index]=pre[:,0].reshape(-1,1)\n",
    "                test_pre[i, :]= model.predict(test_x, num_iteration=model.best_iteration)[:,0].reshape(-1,1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        else:\n",
    "            raise IOError(\"Please add new clf.\")\n",
    "        print(\"%s now score is:\"%clf_name,cv_scores)\n",
    "    test[:]=test_pre.mean(axis=0)\n",
    "    print(\"%s_score_list:\"%clf_name,cv_scores)\n",
    "    print(\"%s_score_mean:\"%clf_name,np.mean(cv_scores))\n",
    "    return train.reshape(-1,1),test.reshape(-1,1)\n",
    "\n",
    "def rf_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    randomforest = RandomForestClassifier(n_estimators=1200, max_depth=20, n_jobs=-1, random_state=2017, max_features=\"auto\",verbose=1)\n",
    "    rf_train, rf_test = stacking_clf(randomforest, x_train, y_train, x_valid, \"rf\", kf, label_split=label_split)\n",
    "    return rf_train, rf_test,\"rf\"\n",
    "\n",
    "def ada_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    adaboost = AdaBoostClassifier(n_estimators=50, random_state=2017, learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking_clf(adaboost, x_train, y_train, x_valid, \"ada\", kf, label_split=label_split)\n",
    "    return ada_train, ada_test,\"ada\"\n",
    "\n",
    "def gb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gbdt = GradientBoostingClassifier(learning_rate=0.04, n_estimators=100, subsample=0.8, random_state=2017,max_depth=5,verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking_clf(gbdt, x_train, y_train, x_valid, \"gb\", kf, label_split=label_split)\n",
    "    return gbdt_train, gbdt_test,\"gb\"\n",
    "\n",
    "def et_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    extratree = ExtraTreesClassifier(n_estimators=1200, max_depth=35, max_features=\"auto\", n_jobs=-1, random_state=2017,verbose=1)\n",
    "    et_train, et_test = stacking_clf(extratree, x_train, y_train, x_valid, \"et\", kf, label_split=label_split)\n",
    "    return et_train, et_test,\"et\"\n",
    "\n",
    "def xgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_clf(xgboost, x_train, y_train, x_valid, \"xgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"xgb\"\n",
    "\n",
    "def lgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_clf(lightgbm, x_train, y_train, x_valid, \"lgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"lgb\"\n",
    "\n",
    "def gnb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gnb=GaussianNB()\n",
    "    gnb_train, gnb_test = stacking_clf(gnb, x_train, y_train, x_valid, \"gnb\", kf, label_split=label_split)\n",
    "    return gnb_train, gnb_test,\"gnb\"\n",
    "\n",
    "def lr_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    logisticregression=LogisticRegression(n_jobs=-1,random_state=2017,C=0.1,max_iter=200)\n",
    "    lr_train, lr_test = stacking_clf(logisticregression, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return lr_train, lr_test, \"lr\"\n",
    "\n",
    "def knn_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    kneighbors=KNeighborsClassifier(n_neighbors=200,n_jobs=-1)\n",
    "    knn_train, knn_test = stacking_clf(kneighbors, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return knn_train, knn_test, \"knn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取训练和验证数据(为stacking特征做准备)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_columns = [c for c in all_data_test.columns if c not in ['label', 'prob', 'seller_path', 'cat_path', 'brand_path', 'action_type_path', 'item_path', 'time_stamp_path']]\n",
    "x_train = all_data_test[~all_data_test['label'].isna()][features_columns].values\n",
    "y_train = all_data_test[~all_data_test['label'].isna()]['label'].values\n",
    "x_valid = all_data_test[all_data_test['label'].isna()][features_columns].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理函数值inf以及nan情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix(data):\n",
    "    where_are_nan = np.isnan(data)\n",
    "    where_are_inf = np.isinf(data)\n",
    "    data[where_are_nan] = 0\n",
    "    data[where_are_inf] = 0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.float_(get_matrix(np.float_(x_train)))\n",
    "y_train = np.int_(y_train)\n",
    "x_valid = x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入划分数据函数 设stacking特征为5折"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "folds = 5\n",
    "seed = 1\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用lgb和xgb分类模型构造stacking特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_list = [lgb_clf, xgb_clf, lgb_reg, xgb_reg]\n",
    "# clf_list_col = ['lgb_clf', 'xgb_clf', 'lgb_reg', 'xgb_reg']\n",
    "\n",
    "clf_list = [lgb_clf, xgb_clf]\n",
    "clf_list_col = ['lgb_clf', 'xgb_clf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型，获取stacking特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 0.240875\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\tvalid_0's multi_logloss: 0.240675\n",
      "[3]\tvalid_0's multi_logloss: 0.240888\n",
      "[4]\tvalid_0's multi_logloss: 0.240865\n",
      "[5]\tvalid_0's multi_logloss: 0.24101\n",
      "[6]\tvalid_0's multi_logloss: 0.241439\n",
      "[7]\tvalid_0's multi_logloss: 0.2416\n",
      "[8]\tvalid_0's multi_logloss: 0.241451\n",
      "[9]\tvalid_0's multi_logloss: 0.241744\n",
      "[10]\tvalid_0's multi_logloss: 0.24179\n",
      "[11]\tvalid_0's multi_logloss: 0.242093\n",
      "[12]\tvalid_0's multi_logloss: 0.242287\n",
      "[13]\tvalid_0's multi_logloss: 0.242484\n",
      "[14]\tvalid_0's multi_logloss: 0.242378\n",
      "[15]\tvalid_0's multi_logloss: 0.242336\n",
      "[16]\tvalid_0's multi_logloss: 0.242333\n",
      "[17]\tvalid_0's multi_logloss: 0.242714\n",
      "[18]\tvalid_0's multi_logloss: 0.24277\n",
      "[19]\tvalid_0's multi_logloss: 0.243274\n",
      "[20]\tvalid_0's multi_logloss: 0.243541\n",
      "[21]\tvalid_0's multi_logloss: 0.243682\n",
      "[22]\tvalid_0's multi_logloss: 0.243973\n",
      "[23]\tvalid_0's multi_logloss: 0.244375\n",
      "[24]\tvalid_0's multi_logloss: 0.24452\n",
      "[25]\tvalid_0's multi_logloss: 0.24475\n",
      "[26]\tvalid_0's multi_logloss: 0.245176\n",
      "[27]\tvalid_0's multi_logloss: 0.245705\n",
      "[28]\tvalid_0's multi_logloss: 0.245993\n",
      "[29]\tvalid_0's multi_logloss: 0.246023\n",
      "[30]\tvalid_0's multi_logloss: 0.246212\n",
      "[31]\tvalid_0's multi_logloss: 0.246423\n",
      "[32]\tvalid_0's multi_logloss: 0.247032\n",
      "[33]\tvalid_0's multi_logloss: 0.247472\n",
      "[34]\tvalid_0's multi_logloss: 0.247535\n",
      "[35]\tvalid_0's multi_logloss: 0.247877\n",
      "[36]\tvalid_0's multi_logloss: 0.247752\n",
      "[37]\tvalid_0's multi_logloss: 0.24794\n",
      "[38]\tvalid_0's multi_logloss: 0.248418\n",
      "[39]\tvalid_0's multi_logloss: 0.248531\n",
      "[40]\tvalid_0's multi_logloss: 0.248795\n",
      "[41]\tvalid_0's multi_logloss: 0.248948\n",
      "[42]\tvalid_0's multi_logloss: 0.249171\n",
      "[43]\tvalid_0's multi_logloss: 0.249188\n",
      "[44]\tvalid_0's multi_logloss: 0.249365\n",
      "[45]\tvalid_0's multi_logloss: 0.249434\n",
      "[46]\tvalid_0's multi_logloss: 0.249295\n",
      "[47]\tvalid_0's multi_logloss: 0.249525\n",
      "[48]\tvalid_0's multi_logloss: 0.249587\n",
      "[49]\tvalid_0's multi_logloss: 0.249906\n",
      "[50]\tvalid_0's multi_logloss: 0.250506\n",
      "[51]\tvalid_0's multi_logloss: 0.250907\n",
      "[52]\tvalid_0's multi_logloss: 0.251236\n",
      "[53]\tvalid_0's multi_logloss: 0.251869\n",
      "[54]\tvalid_0's multi_logloss: 0.251871\n",
      "[55]\tvalid_0's multi_logloss: 0.252232\n",
      "[56]\tvalid_0's multi_logloss: 0.252465\n",
      "[57]\tvalid_0's multi_logloss: 0.252947\n",
      "[58]\tvalid_0's multi_logloss: 0.253515\n",
      "[59]\tvalid_0's multi_logloss: 0.253861\n",
      "[60]\tvalid_0's multi_logloss: 0.254104\n",
      "[61]\tvalid_0's multi_logloss: 0.254456\n",
      "[62]\tvalid_0's multi_logloss: 0.254667\n",
      "[63]\tvalid_0's multi_logloss: 0.254906\n",
      "[64]\tvalid_0's multi_logloss: 0.255107\n",
      "[65]\tvalid_0's multi_logloss: 0.255507\n",
      "[66]\tvalid_0's multi_logloss: 0.25573\n",
      "[67]\tvalid_0's multi_logloss: 0.256335\n",
      "[68]\tvalid_0's multi_logloss: 0.256619\n",
      "[69]\tvalid_0's multi_logloss: 0.256952\n",
      "[70]\tvalid_0's multi_logloss: 0.257173\n",
      "[71]\tvalid_0's multi_logloss: 0.257296\n",
      "[72]\tvalid_0's multi_logloss: 0.257581\n",
      "[73]\tvalid_0's multi_logloss: 0.257646\n",
      "[74]\tvalid_0's multi_logloss: 0.258015\n",
      "[75]\tvalid_0's multi_logloss: 0.258531\n",
      "[76]\tvalid_0's multi_logloss: 0.259022\n",
      "[77]\tvalid_0's multi_logloss: 0.259195\n",
      "[78]\tvalid_0's multi_logloss: 0.259763\n",
      "[79]\tvalid_0's multi_logloss: 0.260062\n",
      "[80]\tvalid_0's multi_logloss: 0.260702\n",
      "[81]\tvalid_0's multi_logloss: 0.261115\n",
      "[82]\tvalid_0's multi_logloss: 0.261478\n",
      "[83]\tvalid_0's multi_logloss: 0.261938\n",
      "[84]\tvalid_0's multi_logloss: 0.262086\n",
      "[85]\tvalid_0's multi_logloss: 0.262411\n",
      "[86]\tvalid_0's multi_logloss: 0.262711\n",
      "[87]\tvalid_0's multi_logloss: 0.262916\n",
      "[88]\tvalid_0's multi_logloss: 0.263369\n",
      "[89]\tvalid_0's multi_logloss: 0.263612\n",
      "[90]\tvalid_0's multi_logloss: 0.264042\n",
      "[91]\tvalid_0's multi_logloss: 0.264335\n",
      "[92]\tvalid_0's multi_logloss: 0.264789\n",
      "[93]\tvalid_0's multi_logloss: 0.26498\n",
      "[94]\tvalid_0's multi_logloss: 0.265366\n",
      "[95]\tvalid_0's multi_logloss: 0.265513\n",
      "[96]\tvalid_0's multi_logloss: 0.265972\n",
      "[97]\tvalid_0's multi_logloss: 0.266151\n",
      "[98]\tvalid_0's multi_logloss: 0.266503\n",
      "[99]\tvalid_0's multi_logloss: 0.266641\n",
      "[100]\tvalid_0's multi_logloss: 0.266948\n",
      "[101]\tvalid_0's multi_logloss: 0.267422\n",
      "[102]\tvalid_0's multi_logloss: 0.267663\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's multi_logloss: 0.240675\n",
      "lgb now score is: [2.581263267601925]\n",
      "[1]\tvalid_0's multi_logloss: 0.281967\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\tvalid_0's multi_logloss: 0.282133\n",
      "[3]\tvalid_0's multi_logloss: 0.282368\n",
      "[4]\tvalid_0's multi_logloss: 0.283084\n",
      "[5]\tvalid_0's multi_logloss: 0.283122\n",
      "[6]\tvalid_0's multi_logloss: 0.283567\n",
      "[7]\tvalid_0's multi_logloss: 0.284085\n",
      "[8]\tvalid_0's multi_logloss: 0.284698\n",
      "[9]\tvalid_0's multi_logloss: 0.285179\n",
      "[10]\tvalid_0's multi_logloss: 0.285581\n",
      "[11]\tvalid_0's multi_logloss: 0.286184\n",
      "[12]\tvalid_0's multi_logloss: 0.286706\n",
      "[13]\tvalid_0's multi_logloss: 0.287115\n",
      "[14]\tvalid_0's multi_logloss: 0.287311\n",
      "[15]\tvalid_0's multi_logloss: 0.28798\n",
      "[16]\tvalid_0's multi_logloss: 0.288391\n",
      "[17]\tvalid_0's multi_logloss: 0.289106\n",
      "[18]\tvalid_0's multi_logloss: 0.289734\n",
      "[19]\tvalid_0's multi_logloss: 0.290529\n",
      "[20]\tvalid_0's multi_logloss: 0.290988\n",
      "[21]\tvalid_0's multi_logloss: 0.291664\n",
      "[22]\tvalid_0's multi_logloss: 0.291645\n",
      "[23]\tvalid_0's multi_logloss: 0.292237\n",
      "[24]\tvalid_0's multi_logloss: 0.292576\n",
      "[25]\tvalid_0's multi_logloss: 0.293331\n",
      "[26]\tvalid_0's multi_logloss: 0.293605\n",
      "[27]\tvalid_0's multi_logloss: 0.293949\n",
      "[28]\tvalid_0's multi_logloss: 0.294226\n",
      "[29]\tvalid_0's multi_logloss: 0.294879\n",
      "[30]\tvalid_0's multi_logloss: 0.295288\n",
      "[31]\tvalid_0's multi_logloss: 0.295465\n",
      "[32]\tvalid_0's multi_logloss: 0.296071\n",
      "[33]\tvalid_0's multi_logloss: 0.296673\n",
      "[34]\tvalid_0's multi_logloss: 0.296436\n",
      "[35]\tvalid_0's multi_logloss: 0.297093\n",
      "[36]\tvalid_0's multi_logloss: 0.297595\n",
      "[37]\tvalid_0's multi_logloss: 0.298183\n",
      "[38]\tvalid_0's multi_logloss: 0.298635\n",
      "[39]\tvalid_0's multi_logloss: 0.298867\n",
      "[40]\tvalid_0's multi_logloss: 0.299411\n",
      "[41]\tvalid_0's multi_logloss: 0.299719\n",
      "[42]\tvalid_0's multi_logloss: 0.300114\n",
      "[43]\tvalid_0's multi_logloss: 0.30074\n",
      "[44]\tvalid_0's multi_logloss: 0.300857\n",
      "[45]\tvalid_0's multi_logloss: 0.301254\n",
      "[46]\tvalid_0's multi_logloss: 0.301502\n",
      "[47]\tvalid_0's multi_logloss: 0.30187\n",
      "[48]\tvalid_0's multi_logloss: 0.302568\n",
      "[49]\tvalid_0's multi_logloss: 0.30307\n",
      "[50]\tvalid_0's multi_logloss: 0.30362\n",
      "[51]\tvalid_0's multi_logloss: 0.304363\n",
      "[52]\tvalid_0's multi_logloss: 0.304392\n",
      "[53]\tvalid_0's multi_logloss: 0.304759\n",
      "[54]\tvalid_0's multi_logloss: 0.305298\n",
      "[55]\tvalid_0's multi_logloss: 0.305868\n",
      "[56]\tvalid_0's multi_logloss: 0.306539\n",
      "[57]\tvalid_0's multi_logloss: 0.307104\n",
      "[58]\tvalid_0's multi_logloss: 0.307546\n",
      "[59]\tvalid_0's multi_logloss: 0.308155\n",
      "[60]\tvalid_0's multi_logloss: 0.308449\n",
      "[61]\tvalid_0's multi_logloss: 0.308901\n",
      "[62]\tvalid_0's multi_logloss: 0.309326\n",
      "[63]\tvalid_0's multi_logloss: 0.309854\n",
      "[64]\tvalid_0's multi_logloss: 0.310218\n",
      "[65]\tvalid_0's multi_logloss: 0.311033\n",
      "[66]\tvalid_0's multi_logloss: 0.311737\n",
      "[67]\tvalid_0's multi_logloss: 0.312168\n",
      "[68]\tvalid_0's multi_logloss: 0.312593\n",
      "[69]\tvalid_0's multi_logloss: 0.313177\n",
      "[70]\tvalid_0's multi_logloss: 0.313479\n",
      "[71]\tvalid_0's multi_logloss: 0.313671\n",
      "[72]\tvalid_0's multi_logloss: 0.314155\n",
      "[73]\tvalid_0's multi_logloss: 0.314633\n",
      "[74]\tvalid_0's multi_logloss: 0.31501\n",
      "[75]\tvalid_0's multi_logloss: 0.315636\n",
      "[76]\tvalid_0's multi_logloss: 0.315862\n",
      "[77]\tvalid_0's multi_logloss: 0.316439\n",
      "[78]\tvalid_0's multi_logloss: 0.316806\n",
      "[79]\tvalid_0's multi_logloss: 0.317156\n",
      "[80]\tvalid_0's multi_logloss: 0.3176\n",
      "[81]\tvalid_0's multi_logloss: 0.318057\n",
      "[82]\tvalid_0's multi_logloss: 0.318501\n",
      "[83]\tvalid_0's multi_logloss: 0.318973\n",
      "[84]\tvalid_0's multi_logloss: 0.319341\n",
      "[85]\tvalid_0's multi_logloss: 0.319797\n",
      "[86]\tvalid_0's multi_logloss: 0.320093\n",
      "[87]\tvalid_0's multi_logloss: 0.320413\n",
      "[88]\tvalid_0's multi_logloss: 0.320675\n",
      "[89]\tvalid_0's multi_logloss: 0.321459\n",
      "[90]\tvalid_0's multi_logloss: 0.321768\n",
      "[91]\tvalid_0's multi_logloss: 0.322247\n",
      "[92]\tvalid_0's multi_logloss: 0.322747\n",
      "[93]\tvalid_0's multi_logloss: 0.323184\n",
      "[94]\tvalid_0's multi_logloss: 0.323553\n",
      "[95]\tvalid_0's multi_logloss: 0.324027\n",
      "[96]\tvalid_0's multi_logloss: 0.324512\n",
      "[97]\tvalid_0's multi_logloss: 0.324828\n",
      "[98]\tvalid_0's multi_logloss: 0.325246\n",
      "[99]\tvalid_0's multi_logloss: 0.325796\n",
      "[100]\tvalid_0's multi_logloss: 0.325952\n",
      "[101]\tvalid_0's multi_logloss: 0.326341\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's multi_logloss: 0.281967\n",
      "lgb now score is: [2.581263267601925, 2.590597701997109]\n",
      "[1]\tvalid_0's multi_logloss: 0.253899\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\tvalid_0's multi_logloss: 0.254055\n",
      "[3]\tvalid_0's multi_logloss: 0.254093\n",
      "[4]\tvalid_0's multi_logloss: 0.254134\n",
      "[5]\tvalid_0's multi_logloss: 0.253928\n",
      "[6]\tvalid_0's multi_logloss: 0.253997\n",
      "[7]\tvalid_0's multi_logloss: 0.253863\n",
      "[8]\tvalid_0's multi_logloss: 0.253992\n",
      "[9]\tvalid_0's multi_logloss: 0.253878\n",
      "[10]\tvalid_0's multi_logloss: 0.253696\n",
      "[11]\tvalid_0's multi_logloss: 0.253385\n",
      "[12]\tvalid_0's multi_logloss: 0.253717\n",
      "[13]\tvalid_0's multi_logloss: 0.253914\n",
      "[14]\tvalid_0's multi_logloss: 0.254158\n",
      "[15]\tvalid_0's multi_logloss: 0.254266\n",
      "[16]\tvalid_0's multi_logloss: 0.254466\n",
      "[17]\tvalid_0's multi_logloss: 0.254132\n",
      "[18]\tvalid_0's multi_logloss: 0.254297\n",
      "[19]\tvalid_0's multi_logloss: 0.25443\n",
      "[20]\tvalid_0's multi_logloss: 0.254878\n",
      "[21]\tvalid_0's multi_logloss: 0.254814\n",
      "[22]\tvalid_0's multi_logloss: 0.254932\n",
      "[23]\tvalid_0's multi_logloss: 0.255018\n",
      "[24]\tvalid_0's multi_logloss: 0.255417\n",
      "[25]\tvalid_0's multi_logloss: 0.255574\n",
      "[26]\tvalid_0's multi_logloss: 0.255596\n",
      "[27]\tvalid_0's multi_logloss: 0.255598\n",
      "[28]\tvalid_0's multi_logloss: 0.255664\n",
      "[29]\tvalid_0's multi_logloss: 0.255873\n",
      "[30]\tvalid_0's multi_logloss: 0.255919\n",
      "[31]\tvalid_0's multi_logloss: 0.256201\n",
      "[32]\tvalid_0's multi_logloss: 0.256339\n",
      "[33]\tvalid_0's multi_logloss: 0.256683\n",
      "[34]\tvalid_0's multi_logloss: 0.256838\n",
      "[35]\tvalid_0's multi_logloss: 0.257111\n",
      "[36]\tvalid_0's multi_logloss: 0.257369\n",
      "[37]\tvalid_0's multi_logloss: 0.257642\n",
      "[38]\tvalid_0's multi_logloss: 0.257903\n",
      "[39]\tvalid_0's multi_logloss: 0.258053\n",
      "[40]\tvalid_0's multi_logloss: 0.258062\n",
      "[41]\tvalid_0's multi_logloss: 0.258296\n",
      "[42]\tvalid_0's multi_logloss: 0.25846\n",
      "[43]\tvalid_0's multi_logloss: 0.258526\n",
      "[44]\tvalid_0's multi_logloss: 0.25868\n",
      "[45]\tvalid_0's multi_logloss: 0.258795\n",
      "[46]\tvalid_0's multi_logloss: 0.258725\n",
      "[47]\tvalid_0's multi_logloss: 0.259161\n",
      "[48]\tvalid_0's multi_logloss: 0.259188\n",
      "[49]\tvalid_0's multi_logloss: 0.259408\n",
      "[50]\tvalid_0's multi_logloss: 0.259687\n",
      "[51]\tvalid_0's multi_logloss: 0.26009\n",
      "[52]\tvalid_0's multi_logloss: 0.26003\n",
      "[53]\tvalid_0's multi_logloss: 0.260533\n",
      "[54]\tvalid_0's multi_logloss: 0.260589\n",
      "[55]\tvalid_0's multi_logloss: 0.260808\n",
      "[56]\tvalid_0's multi_logloss: 0.260619\n",
      "[57]\tvalid_0's multi_logloss: 0.26088\n",
      "[58]\tvalid_0's multi_logloss: 0.26115\n",
      "[59]\tvalid_0's multi_logloss: 0.261412\n",
      "[60]\tvalid_0's multi_logloss: 0.261714\n",
      "[61]\tvalid_0's multi_logloss: 0.262116\n",
      "[62]\tvalid_0's multi_logloss: 0.26239\n",
      "[63]\tvalid_0's multi_logloss: 0.26245\n",
      "[64]\tvalid_0's multi_logloss: 0.262612\n",
      "[65]\tvalid_0's multi_logloss: 0.262792\n",
      "[66]\tvalid_0's multi_logloss: 0.263179\n",
      "[67]\tvalid_0's multi_logloss: 0.263559\n",
      "[68]\tvalid_0's multi_logloss: 0.263806\n",
      "[69]\tvalid_0's multi_logloss: 0.264111\n",
      "[70]\tvalid_0's multi_logloss: 0.264382\n",
      "[71]\tvalid_0's multi_logloss: 0.264662\n",
      "[72]\tvalid_0's multi_logloss: 0.265251\n",
      "[73]\tvalid_0's multi_logloss: 0.265633\n",
      "[74]\tvalid_0's multi_logloss: 0.266052\n",
      "[75]\tvalid_0's multi_logloss: 0.266329\n",
      "[76]\tvalid_0's multi_logloss: 0.266509\n",
      "[77]\tvalid_0's multi_logloss: 0.266782\n",
      "[78]\tvalid_0's multi_logloss: 0.267036\n",
      "[79]\tvalid_0's multi_logloss: 0.267244\n",
      "[80]\tvalid_0's multi_logloss: 0.267406\n",
      "[81]\tvalid_0's multi_logloss: 0.267427\n",
      "[82]\tvalid_0's multi_logloss: 0.267471\n",
      "[83]\tvalid_0's multi_logloss: 0.267694\n",
      "[84]\tvalid_0's multi_logloss: 0.268165\n",
      "[85]\tvalid_0's multi_logloss: 0.268368\n",
      "[86]\tvalid_0's multi_logloss: 0.268544\n",
      "[87]\tvalid_0's multi_logloss: 0.268977\n",
      "[88]\tvalid_0's multi_logloss: 0.269056\n",
      "[89]\tvalid_0's multi_logloss: 0.269396\n",
      "[90]\tvalid_0's multi_logloss: 0.269774\n",
      "[91]\tvalid_0's multi_logloss: 0.270181\n",
      "[92]\tvalid_0's multi_logloss: 0.270548\n",
      "[93]\tvalid_0's multi_logloss: 0.270719\n",
      "[94]\tvalid_0's multi_logloss: 0.270636\n",
      "[95]\tvalid_0's multi_logloss: 0.27076\n",
      "[96]\tvalid_0's multi_logloss: 0.271595\n",
      "[97]\tvalid_0's multi_logloss: 0.271747\n",
      "[98]\tvalid_0's multi_logloss: 0.272099\n",
      "[99]\tvalid_0's multi_logloss: 0.272458\n",
      "[100]\tvalid_0's multi_logloss: 0.272674\n",
      "[101]\tvalid_0's multi_logloss: 0.273034\n",
      "[102]\tvalid_0's multi_logloss: 0.273328\n",
      "[103]\tvalid_0's multi_logloss: 0.273584\n",
      "[104]\tvalid_0's multi_logloss: 0.273789\n",
      "[105]\tvalid_0's multi_logloss: 0.273893\n",
      "[106]\tvalid_0's multi_logloss: 0.274284\n",
      "[107]\tvalid_0's multi_logloss: 0.274575\n",
      "[108]\tvalid_0's multi_logloss: 0.274787\n",
      "[109]\tvalid_0's multi_logloss: 0.275038\n",
      "[110]\tvalid_0's multi_logloss: 0.275374\n",
      "[111]\tvalid_0's multi_logloss: 0.275575\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's multi_logloss: 0.253385\n",
      "lgb now score is: [2.581263267601925, 2.590597701997109, 2.6349821502160853]\n",
      "[1]\tvalid_0's multi_logloss: 0.207865\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\tvalid_0's multi_logloss: 0.207671\n",
      "[3]\tvalid_0's multi_logloss: 0.207825\n",
      "[4]\tvalid_0's multi_logloss: 0.207788\n",
      "[5]\tvalid_0's multi_logloss: 0.207766\n",
      "[6]\tvalid_0's multi_logloss: 0.208017\n",
      "[7]\tvalid_0's multi_logloss: 0.208312\n",
      "[8]\tvalid_0's multi_logloss: 0.20853\n",
      "[9]\tvalid_0's multi_logloss: 0.208613\n",
      "[10]\tvalid_0's multi_logloss: 0.208808\n",
      "[11]\tvalid_0's multi_logloss: 0.209213\n",
      "[12]\tvalid_0's multi_logloss: 0.209337\n",
      "[13]\tvalid_0's multi_logloss: 0.20934\n",
      "[14]\tvalid_0's multi_logloss: 0.209386\n",
      "[15]\tvalid_0's multi_logloss: 0.209685\n",
      "[16]\tvalid_0's multi_logloss: 0.209763\n",
      "[17]\tvalid_0's multi_logloss: 0.209973\n",
      "[18]\tvalid_0's multi_logloss: 0.210005\n",
      "[19]\tvalid_0's multi_logloss: 0.210692\n",
      "[20]\tvalid_0's multi_logloss: 0.210945\n",
      "[21]\tvalid_0's multi_logloss: 0.211078\n",
      "[22]\tvalid_0's multi_logloss: 0.211481\n",
      "[23]\tvalid_0's multi_logloss: 0.211674\n",
      "[24]\tvalid_0's multi_logloss: 0.211416\n",
      "[25]\tvalid_0's multi_logloss: 0.211738\n",
      "[26]\tvalid_0's multi_logloss: 0.211651\n",
      "[27]\tvalid_0's multi_logloss: 0.211965\n",
      "[28]\tvalid_0's multi_logloss: 0.212143\n",
      "[29]\tvalid_0's multi_logloss: 0.212473\n",
      "[30]\tvalid_0's multi_logloss: 0.212534\n",
      "[31]\tvalid_0's multi_logloss: 0.212704\n",
      "[32]\tvalid_0's multi_logloss: 0.21298\n",
      "[33]\tvalid_0's multi_logloss: 0.212932\n",
      "[34]\tvalid_0's multi_logloss: 0.213327\n",
      "[35]\tvalid_0's multi_logloss: 0.213288\n",
      "[36]\tvalid_0's multi_logloss: 0.213633\n",
      "[37]\tvalid_0's multi_logloss: 0.213809\n",
      "[38]\tvalid_0's multi_logloss: 0.213622\n",
      "[39]\tvalid_0's multi_logloss: 0.213885\n",
      "[40]\tvalid_0's multi_logloss: 0.213947\n",
      "[41]\tvalid_0's multi_logloss: 0.214186\n",
      "[42]\tvalid_0's multi_logloss: 0.214596\n",
      "[43]\tvalid_0's multi_logloss: 0.214487\n",
      "[44]\tvalid_0's multi_logloss: 0.214862\n",
      "[45]\tvalid_0's multi_logloss: 0.214739\n",
      "[46]\tvalid_0's multi_logloss: 0.215293\n",
      "[47]\tvalid_0's multi_logloss: 0.215536\n",
      "[48]\tvalid_0's multi_logloss: 0.215667\n",
      "[49]\tvalid_0's multi_logloss: 0.21577\n",
      "[50]\tvalid_0's multi_logloss: 0.215745\n",
      "[51]\tvalid_0's multi_logloss: 0.216011\n",
      "[52]\tvalid_0's multi_logloss: 0.216539\n",
      "[53]\tvalid_0's multi_logloss: 0.216584\n",
      "[54]\tvalid_0's multi_logloss: 0.216415\n",
      "[55]\tvalid_0's multi_logloss: 0.216859\n",
      "[56]\tvalid_0's multi_logloss: 0.217003\n",
      "[57]\tvalid_0's multi_logloss: 0.217141\n",
      "[58]\tvalid_0's multi_logloss: 0.217275\n",
      "[59]\tvalid_0's multi_logloss: 0.217538\n",
      "[60]\tvalid_0's multi_logloss: 0.218079\n",
      "[61]\tvalid_0's multi_logloss: 0.218194\n",
      "[62]\tvalid_0's multi_logloss: 0.218495\n",
      "[63]\tvalid_0's multi_logloss: 0.218899\n",
      "[64]\tvalid_0's multi_logloss: 0.218917\n",
      "[65]\tvalid_0's multi_logloss: 0.218895\n",
      "[66]\tvalid_0's multi_logloss: 0.219267\n",
      "[67]\tvalid_0's multi_logloss: 0.219627\n",
      "[68]\tvalid_0's multi_logloss: 0.219742\n",
      "[69]\tvalid_0's multi_logloss: 0.219754\n",
      "[70]\tvalid_0's multi_logloss: 0.219994\n",
      "[71]\tvalid_0's multi_logloss: 0.220253\n",
      "[72]\tvalid_0's multi_logloss: 0.220718\n",
      "[73]\tvalid_0's multi_logloss: 0.220949\n",
      "[74]\tvalid_0's multi_logloss: 0.22126\n",
      "[75]\tvalid_0's multi_logloss: 0.221389\n",
      "[76]\tvalid_0's multi_logloss: 0.221745\n",
      "[77]\tvalid_0's multi_logloss: 0.221877\n",
      "[78]\tvalid_0's multi_logloss: 0.221702\n",
      "[79]\tvalid_0's multi_logloss: 0.222048\n",
      "[80]\tvalid_0's multi_logloss: 0.222407\n",
      "[81]\tvalid_0's multi_logloss: 0.222611\n",
      "[82]\tvalid_0's multi_logloss: 0.222759\n",
      "[83]\tvalid_0's multi_logloss: 0.223018\n",
      "[84]\tvalid_0's multi_logloss: 0.223229\n",
      "[85]\tvalid_0's multi_logloss: 0.223315\n",
      "[86]\tvalid_0's multi_logloss: 0.223586\n",
      "[87]\tvalid_0's multi_logloss: 0.223966\n",
      "[88]\tvalid_0's multi_logloss: 0.224225\n",
      "[89]\tvalid_0's multi_logloss: 0.22449\n",
      "[90]\tvalid_0's multi_logloss: 0.224714\n",
      "[91]\tvalid_0's multi_logloss: 0.22503\n",
      "[92]\tvalid_0's multi_logloss: 0.22529\n",
      "[93]\tvalid_0's multi_logloss: 0.225522\n",
      "[94]\tvalid_0's multi_logloss: 0.22576\n",
      "[95]\tvalid_0's multi_logloss: 0.225713\n",
      "[96]\tvalid_0's multi_logloss: 0.226025\n",
      "[97]\tvalid_0's multi_logloss: 0.226459\n",
      "[98]\tvalid_0's multi_logloss: 0.226639\n",
      "[99]\tvalid_0's multi_logloss: 0.22676\n",
      "[100]\tvalid_0's multi_logloss: 0.227015\n",
      "[101]\tvalid_0's multi_logloss: 0.227346\n",
      "[102]\tvalid_0's multi_logloss: 0.227715\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's multi_logloss: 0.207671\n",
      "lgb now score is: [2.581263267601925, 2.590597701997109, 2.6349821502160853, 2.565996793434552]\n",
      "[1]\tvalid_0's multi_logloss: 0.214276\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\tvalid_0's multi_logloss: 0.214178\n",
      "[3]\tvalid_0's multi_logloss: 0.214238\n",
      "[4]\tvalid_0's multi_logloss: 0.2144\n",
      "[5]\tvalid_0's multi_logloss: 0.214437\n",
      "[6]\tvalid_0's multi_logloss: 0.214525\n",
      "[7]\tvalid_0's multi_logloss: 0.214297\n",
      "[8]\tvalid_0's multi_logloss: 0.213996\n",
      "[9]\tvalid_0's multi_logloss: 0.214033\n",
      "[10]\tvalid_0's multi_logloss: 0.213869\n",
      "[11]\tvalid_0's multi_logloss: 0.21363\n",
      "[12]\tvalid_0's multi_logloss: 0.213598\n",
      "[13]\tvalid_0's multi_logloss: 0.213387\n",
      "[14]\tvalid_0's multi_logloss: 0.213378\n",
      "[15]\tvalid_0's multi_logloss: 0.213416\n",
      "[16]\tvalid_0's multi_logloss: 0.213521\n",
      "[17]\tvalid_0's multi_logloss: 0.213282\n",
      "[18]\tvalid_0's multi_logloss: 0.213306\n",
      "[19]\tvalid_0's multi_logloss: 0.213519\n",
      "[20]\tvalid_0's multi_logloss: 0.213575\n",
      "[21]\tvalid_0's multi_logloss: 0.213716\n",
      "[22]\tvalid_0's multi_logloss: 0.213659\n",
      "[23]\tvalid_0's multi_logloss: 0.213694\n",
      "[24]\tvalid_0's multi_logloss: 0.213828\n",
      "[25]\tvalid_0's multi_logloss: 0.213992\n",
      "[26]\tvalid_0's multi_logloss: 0.213694\n",
      "[27]\tvalid_0's multi_logloss: 0.213556\n",
      "[28]\tvalid_0's multi_logloss: 0.213837\n",
      "[29]\tvalid_0's multi_logloss: 0.214084\n",
      "[30]\tvalid_0's multi_logloss: 0.214064\n",
      "[31]\tvalid_0's multi_logloss: 0.214209\n",
      "[32]\tvalid_0's multi_logloss: 0.214109\n",
      "[33]\tvalid_0's multi_logloss: 0.214104\n",
      "[34]\tvalid_0's multi_logloss: 0.21415\n",
      "[35]\tvalid_0's multi_logloss: 0.214126\n",
      "[36]\tvalid_0's multi_logloss: 0.214049\n",
      "[37]\tvalid_0's multi_logloss: 0.214361\n",
      "[38]\tvalid_0's multi_logloss: 0.214432\n",
      "[39]\tvalid_0's multi_logloss: 0.214649\n",
      "[40]\tvalid_0's multi_logloss: 0.214613\n",
      "[41]\tvalid_0's multi_logloss: 0.214496\n",
      "[42]\tvalid_0's multi_logloss: 0.214799\n",
      "[43]\tvalid_0's multi_logloss: 0.21498\n",
      "[44]\tvalid_0's multi_logloss: 0.215034\n",
      "[45]\tvalid_0's multi_logloss: 0.215153\n",
      "[46]\tvalid_0's multi_logloss: 0.215297\n",
      "[47]\tvalid_0's multi_logloss: 0.215377\n",
      "[48]\tvalid_0's multi_logloss: 0.215486\n",
      "[49]\tvalid_0's multi_logloss: 0.215718\n",
      "[50]\tvalid_0's multi_logloss: 0.21584\n",
      "[51]\tvalid_0's multi_logloss: 0.216258\n",
      "[52]\tvalid_0's multi_logloss: 0.216591\n",
      "[53]\tvalid_0's multi_logloss: 0.216846\n",
      "[54]\tvalid_0's multi_logloss: 0.216794\n",
      "[55]\tvalid_0's multi_logloss: 0.216667\n",
      "[56]\tvalid_0's multi_logloss: 0.216795\n",
      "[57]\tvalid_0's multi_logloss: 0.216943\n",
      "[58]\tvalid_0's multi_logloss: 0.217151\n",
      "[59]\tvalid_0's multi_logloss: 0.21726\n",
      "[60]\tvalid_0's multi_logloss: 0.217351\n",
      "[61]\tvalid_0's multi_logloss: 0.217576\n",
      "[62]\tvalid_0's multi_logloss: 0.217945\n",
      "[63]\tvalid_0's multi_logloss: 0.218189\n",
      "[64]\tvalid_0's multi_logloss: 0.218539\n",
      "[65]\tvalid_0's multi_logloss: 0.218823\n",
      "[66]\tvalid_0's multi_logloss: 0.219163\n",
      "[67]\tvalid_0's multi_logloss: 0.219334\n",
      "[68]\tvalid_0's multi_logloss: 0.219499\n",
      "[69]\tvalid_0's multi_logloss: 0.219563\n",
      "[70]\tvalid_0's multi_logloss: 0.219675\n",
      "[71]\tvalid_0's multi_logloss: 0.219736\n",
      "[72]\tvalid_0's multi_logloss: 0.219849\n",
      "[73]\tvalid_0's multi_logloss: 0.220197\n",
      "[74]\tvalid_0's multi_logloss: 0.220414\n",
      "[75]\tvalid_0's multi_logloss: 0.220593\n",
      "[76]\tvalid_0's multi_logloss: 0.220923\n",
      "[77]\tvalid_0's multi_logloss: 0.220611\n",
      "[78]\tvalid_0's multi_logloss: 0.220972\n",
      "[79]\tvalid_0's multi_logloss: 0.221091\n",
      "[80]\tvalid_0's multi_logloss: 0.221189\n",
      "[81]\tvalid_0's multi_logloss: 0.22151\n",
      "[82]\tvalid_0's multi_logloss: 0.221571\n",
      "[83]\tvalid_0's multi_logloss: 0.221885\n",
      "[84]\tvalid_0's multi_logloss: 0.222043\n",
      "[85]\tvalid_0's multi_logloss: 0.222272\n",
      "[86]\tvalid_0's multi_logloss: 0.222568\n",
      "[87]\tvalid_0's multi_logloss: 0.222901\n",
      "[88]\tvalid_0's multi_logloss: 0.223321\n",
      "[89]\tvalid_0's multi_logloss: 0.223584\n",
      "[90]\tvalid_0's multi_logloss: 0.223679\n",
      "[91]\tvalid_0's multi_logloss: 0.223784\n",
      "[92]\tvalid_0's multi_logloss: 0.223943\n",
      "[93]\tvalid_0's multi_logloss: 0.224292\n",
      "[94]\tvalid_0's multi_logloss: 0.224652\n",
      "[95]\tvalid_0's multi_logloss: 0.225073\n",
      "[96]\tvalid_0's multi_logloss: 0.225454\n",
      "[97]\tvalid_0's multi_logloss: 0.225778\n",
      "[98]\tvalid_0's multi_logloss: 0.226077\n",
      "[99]\tvalid_0's multi_logloss: 0.226356\n",
      "[100]\tvalid_0's multi_logloss: 0.226567\n",
      "[101]\tvalid_0's multi_logloss: 0.226914\n",
      "[102]\tvalid_0's multi_logloss: 0.227011\n",
      "[103]\tvalid_0's multi_logloss: 0.227502\n",
      "[104]\tvalid_0's multi_logloss: 0.227759\n",
      "[105]\tvalid_0's multi_logloss: 0.227892\n",
      "[106]\tvalid_0's multi_logloss: 0.228322\n",
      "[107]\tvalid_0's multi_logloss: 0.22858\n",
      "[108]\tvalid_0's multi_logloss: 0.228822\n",
      "[109]\tvalid_0's multi_logloss: 0.229067\n",
      "[110]\tvalid_0's multi_logloss: 0.229175\n",
      "[111]\tvalid_0's multi_logloss: 0.229315\n",
      "[112]\tvalid_0's multi_logloss: 0.229414\n",
      "[113]\tvalid_0's multi_logloss: 0.229519\n",
      "[114]\tvalid_0's multi_logloss: 0.229724\n",
      "[115]\tvalid_0's multi_logloss: 0.229994\n",
      "[116]\tvalid_0's multi_logloss: 0.230323\n",
      "[117]\tvalid_0's multi_logloss: 0.230522\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's multi_logloss: 0.213282\n",
      "lgb now score is: [2.581263267601925, 2.590597701997109, 2.6349821502160853, 2.565996793434552, 2.6483075421149227]\n",
      "lgb_score_list: [2.581263267601925, 2.590597701997109, 2.6349821502160853, 2.565996793434552, 2.6483075421149227]\n",
      "lgb_score_mean: 2.604229491072919\n",
      "[0]\ttrain-mlogloss:0.670706\teval-mlogloss:0.671255\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[1]\ttrain-mlogloss:0.649599\teval-mlogloss:0.650497\n",
      "[2]\ttrain-mlogloss:0.629605\teval-mlogloss:0.630682\n",
      "[3]\ttrain-mlogloss:0.610671\teval-mlogloss:0.612022\n",
      "[4]\ttrain-mlogloss:0.592869\teval-mlogloss:0.594536\n",
      "[5]\ttrain-mlogloss:0.57581\teval-mlogloss:0.577855\n",
      "[6]\ttrain-mlogloss:0.559932\teval-mlogloss:0.562436\n",
      "[7]\ttrain-mlogloss:0.544478\teval-mlogloss:0.547365\n",
      "[8]\ttrain-mlogloss:0.530255\teval-mlogloss:0.533735\n",
      "[9]\ttrain-mlogloss:0.516437\teval-mlogloss:0.520181\n",
      "[10]\ttrain-mlogloss:0.503414\teval-mlogloss:0.507377\n",
      "[11]\ttrain-mlogloss:0.490999\teval-mlogloss:0.495439\n",
      "[12]\ttrain-mlogloss:0.479105\teval-mlogloss:0.484005\n",
      "[13]\ttrain-mlogloss:0.467899\teval-mlogloss:0.473487\n",
      "[14]\ttrain-mlogloss:0.457133\teval-mlogloss:0.463292\n",
      "[15]\ttrain-mlogloss:0.446744\teval-mlogloss:0.453165\n",
      "[16]\ttrain-mlogloss:0.436972\teval-mlogloss:0.443767\n",
      "[17]\ttrain-mlogloss:0.427353\teval-mlogloss:0.434672\n",
      "[18]\ttrain-mlogloss:0.418346\teval-mlogloss:0.426254\n",
      "[19]\ttrain-mlogloss:0.409746\teval-mlogloss:0.418001\n",
      "[20]\ttrain-mlogloss:0.401582\teval-mlogloss:0.410063\n",
      "[21]\ttrain-mlogloss:0.393575\teval-mlogloss:0.402683\n",
      "[22]\ttrain-mlogloss:0.38604\teval-mlogloss:0.395728\n",
      "[23]\ttrain-mlogloss:0.378608\teval-mlogloss:0.388837\n",
      "[24]\ttrain-mlogloss:0.371574\teval-mlogloss:0.382203\n",
      "[25]\ttrain-mlogloss:0.364947\teval-mlogloss:0.376021\n",
      "[26]\ttrain-mlogloss:0.358646\teval-mlogloss:0.369892\n",
      "[27]\ttrain-mlogloss:0.352611\teval-mlogloss:0.364415\n",
      "[28]\ttrain-mlogloss:0.346729\teval-mlogloss:0.359147\n",
      "[29]\ttrain-mlogloss:0.340953\teval-mlogloss:0.353912\n",
      "[30]\ttrain-mlogloss:0.335602\teval-mlogloss:0.349052\n",
      "[31]\ttrain-mlogloss:0.330374\teval-mlogloss:0.344226\n",
      "[32]\ttrain-mlogloss:0.325374\teval-mlogloss:0.339553\n",
      "[33]\ttrain-mlogloss:0.320497\teval-mlogloss:0.335416\n",
      "[34]\ttrain-mlogloss:0.315798\teval-mlogloss:0.331179\n",
      "[35]\ttrain-mlogloss:0.311157\teval-mlogloss:0.3272\n",
      "[36]\ttrain-mlogloss:0.306924\teval-mlogloss:0.323553\n",
      "[37]\ttrain-mlogloss:0.302726\teval-mlogloss:0.319867\n",
      "[38]\ttrain-mlogloss:0.298764\teval-mlogloss:0.316447\n",
      "[39]\ttrain-mlogloss:0.294954\teval-mlogloss:0.313232\n",
      "[40]\ttrain-mlogloss:0.291461\teval-mlogloss:0.309973\n",
      "[41]\ttrain-mlogloss:0.288015\teval-mlogloss:0.307126\n",
      "[42]\ttrain-mlogloss:0.284503\teval-mlogloss:0.304089\n",
      "[43]\ttrain-mlogloss:0.281365\teval-mlogloss:0.301478\n",
      "[44]\ttrain-mlogloss:0.278087\teval-mlogloss:0.298756\n",
      "[45]\ttrain-mlogloss:0.275006\teval-mlogloss:0.296098\n",
      "[46]\ttrain-mlogloss:0.27208\teval-mlogloss:0.293981\n",
      "[47]\ttrain-mlogloss:0.269117\teval-mlogloss:0.291525\n",
      "[48]\ttrain-mlogloss:0.266356\teval-mlogloss:0.289314\n",
      "[49]\ttrain-mlogloss:0.263616\teval-mlogloss:0.287153\n",
      "[50]\ttrain-mlogloss:0.261089\teval-mlogloss:0.284995\n",
      "[51]\ttrain-mlogloss:0.258623\teval-mlogloss:0.283149\n",
      "[52]\ttrain-mlogloss:0.256101\teval-mlogloss:0.281423\n",
      "[53]\ttrain-mlogloss:0.253952\teval-mlogloss:0.279766\n",
      "[54]\ttrain-mlogloss:0.251443\teval-mlogloss:0.278079\n",
      "[55]\ttrain-mlogloss:0.24924\teval-mlogloss:0.276471\n",
      "[56]\ttrain-mlogloss:0.247116\teval-mlogloss:0.27507\n",
      "[57]\ttrain-mlogloss:0.244987\teval-mlogloss:0.273499\n",
      "[58]\ttrain-mlogloss:0.242899\teval-mlogloss:0.272126\n",
      "[59]\ttrain-mlogloss:0.240811\teval-mlogloss:0.270962\n",
      "[60]\ttrain-mlogloss:0.238982\teval-mlogloss:0.269886\n",
      "[61]\ttrain-mlogloss:0.236994\teval-mlogloss:0.268732\n",
      "[62]\ttrain-mlogloss:0.2354\teval-mlogloss:0.267589\n",
      "[63]\ttrain-mlogloss:0.233598\teval-mlogloss:0.266435\n",
      "[64]\ttrain-mlogloss:0.231835\teval-mlogloss:0.265204\n",
      "[65]\ttrain-mlogloss:0.230011\teval-mlogloss:0.264235\n",
      "[66]\ttrain-mlogloss:0.2283\teval-mlogloss:0.263484\n",
      "[67]\ttrain-mlogloss:0.226453\teval-mlogloss:0.26251\n",
      "[68]\ttrain-mlogloss:0.224867\teval-mlogloss:0.261933\n",
      "[69]\ttrain-mlogloss:0.223274\teval-mlogloss:0.260922\n",
      "[70]\ttrain-mlogloss:0.221991\teval-mlogloss:0.260126\n",
      "[71]\ttrain-mlogloss:0.220471\teval-mlogloss:0.259448\n",
      "[72]\ttrain-mlogloss:0.218827\teval-mlogloss:0.258802\n",
      "[73]\ttrain-mlogloss:0.217521\teval-mlogloss:0.258078\n",
      "[74]\ttrain-mlogloss:0.216146\teval-mlogloss:0.257578\n",
      "[75]\ttrain-mlogloss:0.214512\teval-mlogloss:0.257222\n",
      "[76]\ttrain-mlogloss:0.213047\teval-mlogloss:0.256675\n",
      "[77]\ttrain-mlogloss:0.211558\teval-mlogloss:0.256418\n",
      "[78]\ttrain-mlogloss:0.210158\teval-mlogloss:0.255918\n",
      "[79]\ttrain-mlogloss:0.20907\teval-mlogloss:0.25536\n",
      "[80]\ttrain-mlogloss:0.207821\teval-mlogloss:0.254784\n",
      "[81]\ttrain-mlogloss:0.206632\teval-mlogloss:0.254539\n",
      "[82]\ttrain-mlogloss:0.205419\teval-mlogloss:0.254289\n",
      "[83]\ttrain-mlogloss:0.204151\teval-mlogloss:0.253888\n",
      "[84]\ttrain-mlogloss:0.202779\teval-mlogloss:0.253576\n",
      "[85]\ttrain-mlogloss:0.201706\teval-mlogloss:0.253392\n",
      "[86]\ttrain-mlogloss:0.200575\teval-mlogloss:0.253205\n",
      "[87]\ttrain-mlogloss:0.199441\teval-mlogloss:0.253084\n",
      "[88]\ttrain-mlogloss:0.198226\teval-mlogloss:0.252941\n",
      "[89]\ttrain-mlogloss:0.197251\teval-mlogloss:0.252726\n",
      "[90]\ttrain-mlogloss:0.196295\teval-mlogloss:0.252415\n",
      "[91]\ttrain-mlogloss:0.19511\teval-mlogloss:0.252497\n",
      "[92]\ttrain-mlogloss:0.193999\teval-mlogloss:0.252265\n",
      "[93]\ttrain-mlogloss:0.192923\teval-mlogloss:0.252022\n",
      "[94]\ttrain-mlogloss:0.191984\teval-mlogloss:0.251761\n",
      "[95]\ttrain-mlogloss:0.190695\teval-mlogloss:0.25172\n",
      "[96]\ttrain-mlogloss:0.189864\teval-mlogloss:0.251563\n",
      "[97]\ttrain-mlogloss:0.188849\teval-mlogloss:0.251502\n",
      "[98]\ttrain-mlogloss:0.18777\teval-mlogloss:0.251633\n",
      "[99]\ttrain-mlogloss:0.186717\teval-mlogloss:0.251671\n",
      "[100]\ttrain-mlogloss:0.185713\teval-mlogloss:0.251603\n",
      "[101]\ttrain-mlogloss:0.184762\teval-mlogloss:0.251504\n",
      "[102]\ttrain-mlogloss:0.183769\teval-mlogloss:0.251437\n",
      "[103]\ttrain-mlogloss:0.182936\teval-mlogloss:0.251487\n",
      "[104]\ttrain-mlogloss:0.181981\teval-mlogloss:0.251346\n",
      "[105]\ttrain-mlogloss:0.18104\teval-mlogloss:0.251312\n",
      "[106]\ttrain-mlogloss:0.180098\teval-mlogloss:0.251152\n",
      "[107]\ttrain-mlogloss:0.179284\teval-mlogloss:0.251095\n",
      "[108]\ttrain-mlogloss:0.178376\teval-mlogloss:0.25142\n",
      "[109]\ttrain-mlogloss:0.177675\teval-mlogloss:0.251661\n",
      "[110]\ttrain-mlogloss:0.176651\teval-mlogloss:0.251924\n",
      "[111]\ttrain-mlogloss:0.176106\teval-mlogloss:0.251784\n",
      "[112]\ttrain-mlogloss:0.175328\teval-mlogloss:0.251685\n",
      "[113]\ttrain-mlogloss:0.174529\teval-mlogloss:0.251574\n",
      "[114]\ttrain-mlogloss:0.173635\teval-mlogloss:0.251595\n",
      "[115]\ttrain-mlogloss:0.172948\teval-mlogloss:0.251782\n",
      "[116]\ttrain-mlogloss:0.172322\teval-mlogloss:0.251958\n",
      "[117]\ttrain-mlogloss:0.171539\teval-mlogloss:0.252014\n",
      "[118]\ttrain-mlogloss:0.170762\teval-mlogloss:0.252208\n",
      "[119]\ttrain-mlogloss:0.169995\teval-mlogloss:0.252272\n",
      "[120]\ttrain-mlogloss:0.169196\teval-mlogloss:0.252199\n",
      "[121]\ttrain-mlogloss:0.168431\teval-mlogloss:0.252398\n",
      "[122]\ttrain-mlogloss:0.16755\teval-mlogloss:0.252591\n",
      "[123]\ttrain-mlogloss:0.166925\teval-mlogloss:0.252596\n",
      "[124]\ttrain-mlogloss:0.166264\teval-mlogloss:0.252709\n",
      "[125]\ttrain-mlogloss:0.16567\teval-mlogloss:0.252735\n",
      "[126]\ttrain-mlogloss:0.164993\teval-mlogloss:0.252717\n",
      "[127]\ttrain-mlogloss:0.164187\teval-mlogloss:0.252821\n",
      "[128]\ttrain-mlogloss:0.163574\teval-mlogloss:0.252723\n",
      "[129]\ttrain-mlogloss:0.162853\teval-mlogloss:0.252655\n",
      "[130]\ttrain-mlogloss:0.162043\teval-mlogloss:0.252706\n",
      "[131]\ttrain-mlogloss:0.161511\teval-mlogloss:0.25281\n",
      "[132]\ttrain-mlogloss:0.160613\teval-mlogloss:0.253054\n",
      "[133]\ttrain-mlogloss:0.15987\teval-mlogloss:0.252855\n",
      "[134]\ttrain-mlogloss:0.159088\teval-mlogloss:0.253125\n",
      "[135]\ttrain-mlogloss:0.158449\teval-mlogloss:0.253135\n",
      "[136]\ttrain-mlogloss:0.15774\teval-mlogloss:0.253583\n",
      "[137]\ttrain-mlogloss:0.157081\teval-mlogloss:0.253785\n",
      "[138]\ttrain-mlogloss:0.156546\teval-mlogloss:0.253751\n",
      "[139]\ttrain-mlogloss:0.155805\teval-mlogloss:0.253912\n",
      "[140]\ttrain-mlogloss:0.155167\teval-mlogloss:0.254232\n",
      "[141]\ttrain-mlogloss:0.154506\teval-mlogloss:0.254438\n",
      "[142]\ttrain-mlogloss:0.153914\teval-mlogloss:0.254639\n",
      "[143]\ttrain-mlogloss:0.15328\teval-mlogloss:0.254863\n",
      "[144]\ttrain-mlogloss:0.15264\teval-mlogloss:0.255053\n",
      "[145]\ttrain-mlogloss:0.15189\teval-mlogloss:0.255365\n",
      "[146]\ttrain-mlogloss:0.151272\teval-mlogloss:0.255417\n",
      "[147]\ttrain-mlogloss:0.150644\teval-mlogloss:0.255536\n",
      "[148]\ttrain-mlogloss:0.150066\teval-mlogloss:0.255714\n",
      "[149]\ttrain-mlogloss:0.149465\teval-mlogloss:0.25583\n",
      "[150]\ttrain-mlogloss:0.148941\teval-mlogloss:0.256093\n",
      "[151]\ttrain-mlogloss:0.148275\teval-mlogloss:0.256374\n",
      "[152]\ttrain-mlogloss:0.147665\teval-mlogloss:0.256626\n",
      "[153]\ttrain-mlogloss:0.147092\teval-mlogloss:0.256734\n",
      "[154]\ttrain-mlogloss:0.146628\teval-mlogloss:0.256678\n",
      "[155]\ttrain-mlogloss:0.146188\teval-mlogloss:0.256723\n",
      "[156]\ttrain-mlogloss:0.145616\teval-mlogloss:0.256761\n",
      "[157]\ttrain-mlogloss:0.145017\teval-mlogloss:0.256947\n",
      "[158]\ttrain-mlogloss:0.144546\teval-mlogloss:0.256891\n",
      "[159]\ttrain-mlogloss:0.143883\teval-mlogloss:0.257068\n",
      "[160]\ttrain-mlogloss:0.143318\teval-mlogloss:0.257251\n",
      "[161]\ttrain-mlogloss:0.142735\teval-mlogloss:0.257497\n",
      "[162]\ttrain-mlogloss:0.142178\teval-mlogloss:0.257637\n",
      "[163]\ttrain-mlogloss:0.141671\teval-mlogloss:0.257785\n",
      "[164]\ttrain-mlogloss:0.141191\teval-mlogloss:0.257857\n",
      "[165]\ttrain-mlogloss:0.140734\teval-mlogloss:0.257918\n",
      "[166]\ttrain-mlogloss:0.140214\teval-mlogloss:0.257922\n",
      "[167]\ttrain-mlogloss:0.139754\teval-mlogloss:0.258158\n",
      "[168]\ttrain-mlogloss:0.139214\teval-mlogloss:0.258136\n",
      "[169]\ttrain-mlogloss:0.138665\teval-mlogloss:0.258375\n",
      "[170]\ttrain-mlogloss:0.138154\teval-mlogloss:0.258575\n",
      "[171]\ttrain-mlogloss:0.137718\teval-mlogloss:0.258806\n",
      "[172]\ttrain-mlogloss:0.137249\teval-mlogloss:0.258833\n",
      "[173]\ttrain-mlogloss:0.136837\teval-mlogloss:0.258931\n",
      "[174]\ttrain-mlogloss:0.136205\teval-mlogloss:0.258906\n",
      "[175]\ttrain-mlogloss:0.13575\teval-mlogloss:0.259188\n",
      "[176]\ttrain-mlogloss:0.1352\teval-mlogloss:0.25933\n",
      "[177]\ttrain-mlogloss:0.134705\teval-mlogloss:0.259405\n",
      "[178]\ttrain-mlogloss:0.134154\teval-mlogloss:0.259711\n",
      "[179]\ttrain-mlogloss:0.133681\teval-mlogloss:0.259871\n",
      "[180]\ttrain-mlogloss:0.133281\teval-mlogloss:0.259925\n",
      "[181]\ttrain-mlogloss:0.132708\teval-mlogloss:0.260392\n",
      "[182]\ttrain-mlogloss:0.132295\teval-mlogloss:0.26038\n",
      "[183]\ttrain-mlogloss:0.131868\teval-mlogloss:0.260638\n",
      "[184]\ttrain-mlogloss:0.131466\teval-mlogloss:0.260835\n",
      "[185]\ttrain-mlogloss:0.130991\teval-mlogloss:0.261019\n",
      "[186]\ttrain-mlogloss:0.130574\teval-mlogloss:0.26122\n",
      "[187]\ttrain-mlogloss:0.130131\teval-mlogloss:0.261402\n",
      "[188]\ttrain-mlogloss:0.129676\teval-mlogloss:0.261529\n",
      "[189]\ttrain-mlogloss:0.129184\teval-mlogloss:0.261684\n",
      "[190]\ttrain-mlogloss:0.128745\teval-mlogloss:0.2617\n",
      "[191]\ttrain-mlogloss:0.128322\teval-mlogloss:0.261794\n",
      "[192]\ttrain-mlogloss:0.127868\teval-mlogloss:0.261726\n",
      "[193]\ttrain-mlogloss:0.127488\teval-mlogloss:0.261829\n",
      "[194]\ttrain-mlogloss:0.127113\teval-mlogloss:0.261933\n",
      "[195]\ttrain-mlogloss:0.126671\teval-mlogloss:0.262253\n",
      "[196]\ttrain-mlogloss:0.126286\teval-mlogloss:0.262373\n",
      "[197]\ttrain-mlogloss:0.12585\teval-mlogloss:0.262263\n",
      "[198]\ttrain-mlogloss:0.12551\teval-mlogloss:0.262553\n",
      "[199]\ttrain-mlogloss:0.125141\teval-mlogloss:0.262671\n",
      "[200]\ttrain-mlogloss:0.124814\teval-mlogloss:0.26299\n",
      "[201]\ttrain-mlogloss:0.124441\teval-mlogloss:0.263402\n",
      "[202]\ttrain-mlogloss:0.124032\teval-mlogloss:0.263812\n",
      "[203]\ttrain-mlogloss:0.123619\teval-mlogloss:0.263948\n",
      "[204]\ttrain-mlogloss:0.123256\teval-mlogloss:0.263915\n",
      "[205]\ttrain-mlogloss:0.122937\teval-mlogloss:0.263953\n",
      "[206]\ttrain-mlogloss:0.122572\teval-mlogloss:0.264053\n",
      "[207]\ttrain-mlogloss:0.122124\teval-mlogloss:0.263994\n",
      "Stopping. Best iteration:\n",
      "[107]\ttrain-mlogloss:0.179284\teval-mlogloss:0.251095\n",
      "\n",
      "xgb now score is: [2.4208301225770263]\n",
      "[0]\ttrain-mlogloss:0.67062\teval-mlogloss:0.672236\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[1]\ttrain-mlogloss:0.649348\teval-mlogloss:0.65233\n",
      "[2]\ttrain-mlogloss:0.629034\teval-mlogloss:0.633431\n",
      "[3]\ttrain-mlogloss:0.609832\teval-mlogloss:0.615454\n",
      "[4]\ttrain-mlogloss:0.591599\teval-mlogloss:0.598696\n",
      "[5]\ttrain-mlogloss:0.574399\teval-mlogloss:0.582747\n",
      "[6]\ttrain-mlogloss:0.558025\teval-mlogloss:0.567625\n",
      "[7]\ttrain-mlogloss:0.542496\teval-mlogloss:0.553384\n",
      "[8]\ttrain-mlogloss:0.527645\teval-mlogloss:0.539785\n",
      "[9]\ttrain-mlogloss:0.513703\teval-mlogloss:0.527099\n",
      "[10]\ttrain-mlogloss:0.500301\teval-mlogloss:0.514997\n",
      "[11]\ttrain-mlogloss:0.487667\teval-mlogloss:0.503451\n",
      "[12]\ttrain-mlogloss:0.475598\teval-mlogloss:0.492825\n",
      "[13]\ttrain-mlogloss:0.463958\teval-mlogloss:0.482333\n",
      "[14]\ttrain-mlogloss:0.452929\teval-mlogloss:0.472518\n",
      "[15]\ttrain-mlogloss:0.442466\teval-mlogloss:0.46311\n",
      "[16]\ttrain-mlogloss:0.432351\teval-mlogloss:0.454194\n",
      "[17]\ttrain-mlogloss:0.422773\teval-mlogloss:0.445681\n",
      "[18]\ttrain-mlogloss:0.413491\teval-mlogloss:0.437587\n",
      "[19]\ttrain-mlogloss:0.404714\teval-mlogloss:0.429933\n",
      "[20]\ttrain-mlogloss:0.396153\teval-mlogloss:0.422549\n",
      "[21]\ttrain-mlogloss:0.388113\teval-mlogloss:0.415519\n",
      "[22]\ttrain-mlogloss:0.38033\teval-mlogloss:0.408981\n",
      "[23]\ttrain-mlogloss:0.372939\teval-mlogloss:0.402749\n",
      "[24]\ttrain-mlogloss:0.365805\teval-mlogloss:0.396583\n",
      "[25]\ttrain-mlogloss:0.358958\teval-mlogloss:0.39082\n",
      "[26]\ttrain-mlogloss:0.352446\teval-mlogloss:0.385301\n",
      "[27]\ttrain-mlogloss:0.346255\teval-mlogloss:0.380486\n",
      "[28]\ttrain-mlogloss:0.340215\teval-mlogloss:0.37547\n",
      "[29]\ttrain-mlogloss:0.334451\teval-mlogloss:0.370579\n",
      "[30]\ttrain-mlogloss:0.32886\teval-mlogloss:0.365966\n",
      "[31]\ttrain-mlogloss:0.323601\teval-mlogloss:0.361731\n",
      "[32]\ttrain-mlogloss:0.318525\teval-mlogloss:0.357666\n",
      "[33]\ttrain-mlogloss:0.313631\teval-mlogloss:0.354138\n",
      "[34]\ttrain-mlogloss:0.30888\teval-mlogloss:0.350244\n",
      "[35]\ttrain-mlogloss:0.30441\teval-mlogloss:0.346711\n",
      "[36]\ttrain-mlogloss:0.300129\teval-mlogloss:0.343392\n",
      "[37]\ttrain-mlogloss:0.295921\teval-mlogloss:0.340036\n",
      "[38]\ttrain-mlogloss:0.291849\teval-mlogloss:0.337002\n",
      "[39]\ttrain-mlogloss:0.288004\teval-mlogloss:0.333988\n",
      "[40]\ttrain-mlogloss:0.284352\teval-mlogloss:0.331277\n",
      "[41]\ttrain-mlogloss:0.280652\teval-mlogloss:0.328921\n",
      "[42]\ttrain-mlogloss:0.277231\teval-mlogloss:0.32651\n",
      "[43]\ttrain-mlogloss:0.273776\teval-mlogloss:0.324176\n",
      "[44]\ttrain-mlogloss:0.270423\teval-mlogloss:0.322\n",
      "[45]\ttrain-mlogloss:0.267379\teval-mlogloss:0.31982\n",
      "[46]\ttrain-mlogloss:0.264257\teval-mlogloss:0.317643\n",
      "[47]\ttrain-mlogloss:0.26129\teval-mlogloss:0.315604\n",
      "[48]\ttrain-mlogloss:0.258382\teval-mlogloss:0.313659\n",
      "[49]\ttrain-mlogloss:0.255667\teval-mlogloss:0.31194\n",
      "[50]\ttrain-mlogloss:0.253173\teval-mlogloss:0.310158\n",
      "[51]\ttrain-mlogloss:0.250714\teval-mlogloss:0.308744\n",
      "[52]\ttrain-mlogloss:0.248237\teval-mlogloss:0.307435\n",
      "[53]\ttrain-mlogloss:0.246052\teval-mlogloss:0.306165\n",
      "[54]\ttrain-mlogloss:0.243636\teval-mlogloss:0.304897\n",
      "[55]\ttrain-mlogloss:0.241336\teval-mlogloss:0.303716\n",
      "[56]\ttrain-mlogloss:0.239287\teval-mlogloss:0.302434\n",
      "[57]\ttrain-mlogloss:0.237221\teval-mlogloss:0.301253\n",
      "[58]\ttrain-mlogloss:0.235315\teval-mlogloss:0.300104\n",
      "[59]\ttrain-mlogloss:0.23332\teval-mlogloss:0.299265\n",
      "[60]\ttrain-mlogloss:0.231231\teval-mlogloss:0.298395\n",
      "[61]\ttrain-mlogloss:0.229107\teval-mlogloss:0.297615\n",
      "[62]\ttrain-mlogloss:0.227039\teval-mlogloss:0.297156\n",
      "[63]\ttrain-mlogloss:0.225268\teval-mlogloss:0.296373\n",
      "[64]\ttrain-mlogloss:0.223508\teval-mlogloss:0.29563\n",
      "[65]\ttrain-mlogloss:0.221917\teval-mlogloss:0.294999\n",
      "[66]\ttrain-mlogloss:0.220233\teval-mlogloss:0.294356\n",
      "[67]\ttrain-mlogloss:0.218423\teval-mlogloss:0.293856\n",
      "[68]\ttrain-mlogloss:0.216896\teval-mlogloss:0.293587\n",
      "[69]\ttrain-mlogloss:0.215373\teval-mlogloss:0.293066\n",
      "[70]\ttrain-mlogloss:0.213906\teval-mlogloss:0.292657\n",
      "[71]\ttrain-mlogloss:0.212364\teval-mlogloss:0.292382\n",
      "[72]\ttrain-mlogloss:0.210921\teval-mlogloss:0.292094\n",
      "[73]\ttrain-mlogloss:0.209482\teval-mlogloss:0.291805\n",
      "[74]\ttrain-mlogloss:0.208264\teval-mlogloss:0.291376\n",
      "[75]\ttrain-mlogloss:0.206839\teval-mlogloss:0.291069\n",
      "[76]\ttrain-mlogloss:0.205609\teval-mlogloss:0.290981\n",
      "[77]\ttrain-mlogloss:0.20426\teval-mlogloss:0.290716\n",
      "[78]\ttrain-mlogloss:0.202988\teval-mlogloss:0.290575\n",
      "[79]\ttrain-mlogloss:0.201564\teval-mlogloss:0.290414\n",
      "[80]\ttrain-mlogloss:0.200174\teval-mlogloss:0.29032\n",
      "[81]\ttrain-mlogloss:0.199012\teval-mlogloss:0.290308\n",
      "[82]\ttrain-mlogloss:0.197835\teval-mlogloss:0.29047\n",
      "[83]\ttrain-mlogloss:0.19643\teval-mlogloss:0.290346\n",
      "[84]\ttrain-mlogloss:0.195218\teval-mlogloss:0.290325\n",
      "[85]\ttrain-mlogloss:0.194219\teval-mlogloss:0.290215\n",
      "[86]\ttrain-mlogloss:0.192996\teval-mlogloss:0.289965\n",
      "[87]\ttrain-mlogloss:0.191919\teval-mlogloss:0.289688\n",
      "[88]\ttrain-mlogloss:0.19081\teval-mlogloss:0.289625\n",
      "[89]\ttrain-mlogloss:0.189611\teval-mlogloss:0.289864\n",
      "[90]\ttrain-mlogloss:0.188599\teval-mlogloss:0.289836\n",
      "[91]\ttrain-mlogloss:0.187411\teval-mlogloss:0.289955\n",
      "[92]\ttrain-mlogloss:0.186401\teval-mlogloss:0.290123\n",
      "[93]\ttrain-mlogloss:0.185357\teval-mlogloss:0.290303\n",
      "[94]\ttrain-mlogloss:0.184344\teval-mlogloss:0.290303\n",
      "[95]\ttrain-mlogloss:0.183335\teval-mlogloss:0.290291\n",
      "[96]\ttrain-mlogloss:0.18248\teval-mlogloss:0.290669\n",
      "[97]\ttrain-mlogloss:0.181453\teval-mlogloss:0.290946\n",
      "[98]\ttrain-mlogloss:0.180597\teval-mlogloss:0.291298\n",
      "[99]\ttrain-mlogloss:0.179828\teval-mlogloss:0.291479\n",
      "[100]\ttrain-mlogloss:0.178983\teval-mlogloss:0.291725\n",
      "[101]\ttrain-mlogloss:0.177977\teval-mlogloss:0.291756\n",
      "[102]\ttrain-mlogloss:0.176971\teval-mlogloss:0.291978\n",
      "[103]\ttrain-mlogloss:0.176001\teval-mlogloss:0.292205\n",
      "[104]\ttrain-mlogloss:0.175046\teval-mlogloss:0.292321\n",
      "[105]\ttrain-mlogloss:0.174244\teval-mlogloss:0.292433\n",
      "[106]\ttrain-mlogloss:0.173394\teval-mlogloss:0.292576\n",
      "[107]\ttrain-mlogloss:0.172489\teval-mlogloss:0.292693\n",
      "[108]\ttrain-mlogloss:0.171654\teval-mlogloss:0.292819\n",
      "[109]\ttrain-mlogloss:0.170829\teval-mlogloss:0.292828\n",
      "[110]\ttrain-mlogloss:0.169934\teval-mlogloss:0.293139\n",
      "[111]\ttrain-mlogloss:0.16886\teval-mlogloss:0.293524\n",
      "[112]\ttrain-mlogloss:0.167959\teval-mlogloss:0.293939\n",
      "[113]\ttrain-mlogloss:0.167156\teval-mlogloss:0.294132\n",
      "[114]\ttrain-mlogloss:0.166373\teval-mlogloss:0.294386\n",
      "[115]\ttrain-mlogloss:0.16553\teval-mlogloss:0.294737\n",
      "[116]\ttrain-mlogloss:0.164691\teval-mlogloss:0.294786\n",
      "[117]\ttrain-mlogloss:0.163902\teval-mlogloss:0.294932\n",
      "[118]\ttrain-mlogloss:0.163092\teval-mlogloss:0.295323\n",
      "[119]\ttrain-mlogloss:0.162394\teval-mlogloss:0.295658\n",
      "[120]\ttrain-mlogloss:0.161556\teval-mlogloss:0.295982\n",
      "[121]\ttrain-mlogloss:0.160942\teval-mlogloss:0.296354\n",
      "[122]\ttrain-mlogloss:0.160141\teval-mlogloss:0.296614\n",
      "[123]\ttrain-mlogloss:0.159288\teval-mlogloss:0.29686\n",
      "[124]\ttrain-mlogloss:0.158502\teval-mlogloss:0.297339\n",
      "[125]\ttrain-mlogloss:0.157756\teval-mlogloss:0.297783\n",
      "[126]\ttrain-mlogloss:0.157065\teval-mlogloss:0.298368\n",
      "[127]\ttrain-mlogloss:0.156476\teval-mlogloss:0.298653\n",
      "[128]\ttrain-mlogloss:0.155632\teval-mlogloss:0.299199\n",
      "[129]\ttrain-mlogloss:0.154952\teval-mlogloss:0.299515\n",
      "[130]\ttrain-mlogloss:0.154129\teval-mlogloss:0.299856\n",
      "[131]\ttrain-mlogloss:0.15332\teval-mlogloss:0.2999\n",
      "[132]\ttrain-mlogloss:0.15283\teval-mlogloss:0.300204\n",
      "[133]\ttrain-mlogloss:0.152155\teval-mlogloss:0.300502\n",
      "[134]\ttrain-mlogloss:0.151392\teval-mlogloss:0.300858\n",
      "[135]\ttrain-mlogloss:0.150878\teval-mlogloss:0.301139\n",
      "[136]\ttrain-mlogloss:0.150155\teval-mlogloss:0.301651\n",
      "[137]\ttrain-mlogloss:0.149636\teval-mlogloss:0.301839\n",
      "[138]\ttrain-mlogloss:0.148896\teval-mlogloss:0.302246\n",
      "[139]\ttrain-mlogloss:0.148365\teval-mlogloss:0.30273\n",
      "[140]\ttrain-mlogloss:0.147796\teval-mlogloss:0.303003\n",
      "[141]\ttrain-mlogloss:0.147211\teval-mlogloss:0.303374\n",
      "[142]\ttrain-mlogloss:0.14671\teval-mlogloss:0.303504\n",
      "[143]\ttrain-mlogloss:0.146144\teval-mlogloss:0.30381\n",
      "[144]\ttrain-mlogloss:0.145481\teval-mlogloss:0.30443\n",
      "[145]\ttrain-mlogloss:0.144765\teval-mlogloss:0.304772\n",
      "[146]\ttrain-mlogloss:0.144234\teval-mlogloss:0.304989\n",
      "[147]\ttrain-mlogloss:0.143699\teval-mlogloss:0.305134\n",
      "[148]\ttrain-mlogloss:0.143049\teval-mlogloss:0.305576\n",
      "[149]\ttrain-mlogloss:0.142497\teval-mlogloss:0.306114\n",
      "[150]\ttrain-mlogloss:0.141877\teval-mlogloss:0.306513\n",
      "[151]\ttrain-mlogloss:0.14129\teval-mlogloss:0.306837\n",
      "[152]\ttrain-mlogloss:0.140895\teval-mlogloss:0.307303\n",
      "[153]\ttrain-mlogloss:0.140307\teval-mlogloss:0.307763\n",
      "[154]\ttrain-mlogloss:0.139819\teval-mlogloss:0.308032\n",
      "[155]\ttrain-mlogloss:0.139294\teval-mlogloss:0.307971\n",
      "[156]\ttrain-mlogloss:0.138823\teval-mlogloss:0.308253\n",
      "[157]\ttrain-mlogloss:0.138212\teval-mlogloss:0.308723\n",
      "[158]\ttrain-mlogloss:0.13769\teval-mlogloss:0.309165\n",
      "[159]\ttrain-mlogloss:0.137251\teval-mlogloss:0.30962\n",
      "[160]\ttrain-mlogloss:0.136751\teval-mlogloss:0.309856\n",
      "[161]\ttrain-mlogloss:0.1362\teval-mlogloss:0.310249\n",
      "[162]\ttrain-mlogloss:0.135761\teval-mlogloss:0.310294\n",
      "[163]\ttrain-mlogloss:0.135147\teval-mlogloss:0.310319\n",
      "[164]\ttrain-mlogloss:0.13456\teval-mlogloss:0.310788\n",
      "[165]\ttrain-mlogloss:0.134104\teval-mlogloss:0.3111\n",
      "[166]\ttrain-mlogloss:0.133573\teval-mlogloss:0.311249\n",
      "[167]\ttrain-mlogloss:0.133064\teval-mlogloss:0.311454\n",
      "[168]\ttrain-mlogloss:0.132507\teval-mlogloss:0.311765\n",
      "[169]\ttrain-mlogloss:0.132017\teval-mlogloss:0.312056\n",
      "[170]\ttrain-mlogloss:0.131442\teval-mlogloss:0.312534\n",
      "[171]\ttrain-mlogloss:0.130942\teval-mlogloss:0.312956\n",
      "[172]\ttrain-mlogloss:0.130391\teval-mlogloss:0.313142\n",
      "[173]\ttrain-mlogloss:0.129871\teval-mlogloss:0.313077\n",
      "[174]\ttrain-mlogloss:0.129394\teval-mlogloss:0.31347\n",
      "[175]\ttrain-mlogloss:0.128952\teval-mlogloss:0.313803\n",
      "[176]\ttrain-mlogloss:0.12838\teval-mlogloss:0.314011\n",
      "[177]\ttrain-mlogloss:0.127935\teval-mlogloss:0.31402\n",
      "[178]\ttrain-mlogloss:0.127549\teval-mlogloss:0.314281\n",
      "[179]\ttrain-mlogloss:0.127093\teval-mlogloss:0.314691\n",
      "[180]\ttrain-mlogloss:0.126753\teval-mlogloss:0.314796\n",
      "[181]\ttrain-mlogloss:0.126322\teval-mlogloss:0.315402\n",
      "[182]\ttrain-mlogloss:0.125812\teval-mlogloss:0.315699\n",
      "[183]\ttrain-mlogloss:0.125398\teval-mlogloss:0.316155\n",
      "[184]\ttrain-mlogloss:0.12496\teval-mlogloss:0.316433\n",
      "[185]\ttrain-mlogloss:0.124651\teval-mlogloss:0.316687\n",
      "[186]\ttrain-mlogloss:0.12424\teval-mlogloss:0.316884\n",
      "[187]\ttrain-mlogloss:0.123864\teval-mlogloss:0.317156\n",
      "[188]\ttrain-mlogloss:0.12343\teval-mlogloss:0.317291\n",
      "Stopping. Best iteration:\n",
      "[88]\ttrain-mlogloss:0.19081\teval-mlogloss:0.289625\n",
      "\n",
      "xgb now score is: [2.4208301225770263, 2.2433633135072886]\n",
      "[0]\ttrain-mlogloss:0.670717\teval-mlogloss:0.671445\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[1]\ttrain-mlogloss:0.649247\teval-mlogloss:0.650842\n",
      "[2]\ttrain-mlogloss:0.628924\teval-mlogloss:0.631395\n",
      "[3]\ttrain-mlogloss:0.609743\teval-mlogloss:0.612813\n",
      "[4]\ttrain-mlogloss:0.591539\teval-mlogloss:0.595118\n",
      "[5]\ttrain-mlogloss:0.574475\teval-mlogloss:0.579017\n",
      "[6]\ttrain-mlogloss:0.558451\teval-mlogloss:0.563723\n",
      "[7]\ttrain-mlogloss:0.54314\teval-mlogloss:0.548978\n",
      "[8]\ttrain-mlogloss:0.528546\teval-mlogloss:0.535224\n",
      "[9]\ttrain-mlogloss:0.514811\teval-mlogloss:0.522183\n",
      "[10]\ttrain-mlogloss:0.501818\teval-mlogloss:0.509868\n",
      "[11]\ttrain-mlogloss:0.489254\teval-mlogloss:0.498041\n",
      "[12]\ttrain-mlogloss:0.477392\teval-mlogloss:0.486864\n",
      "[13]\ttrain-mlogloss:0.465776\teval-mlogloss:0.47595\n",
      "[14]\ttrain-mlogloss:0.454906\teval-mlogloss:0.46578\n",
      "[15]\ttrain-mlogloss:0.444347\teval-mlogloss:0.455955\n",
      "[16]\ttrain-mlogloss:0.434462\teval-mlogloss:0.44677\n",
      "[17]\ttrain-mlogloss:0.424847\teval-mlogloss:0.437868\n",
      "[18]\ttrain-mlogloss:0.415673\teval-mlogloss:0.429373\n",
      "[19]\ttrain-mlogloss:0.406982\teval-mlogloss:0.421227\n",
      "[20]\ttrain-mlogloss:0.398818\teval-mlogloss:0.413638\n",
      "[21]\ttrain-mlogloss:0.390985\teval-mlogloss:0.406341\n",
      "[22]\ttrain-mlogloss:0.383432\teval-mlogloss:0.399382\n",
      "[23]\ttrain-mlogloss:0.376237\teval-mlogloss:0.392963\n",
      "[24]\ttrain-mlogloss:0.369184\teval-mlogloss:0.386478\n",
      "[25]\ttrain-mlogloss:0.362454\teval-mlogloss:0.380372\n",
      "[26]\ttrain-mlogloss:0.356107\teval-mlogloss:0.374598\n",
      "[27]\ttrain-mlogloss:0.349908\teval-mlogloss:0.369184\n",
      "[28]\ttrain-mlogloss:0.344016\teval-mlogloss:0.36397\n",
      "[29]\ttrain-mlogloss:0.338286\teval-mlogloss:0.358962\n",
      "[30]\ttrain-mlogloss:0.333035\teval-mlogloss:0.35421\n",
      "[31]\ttrain-mlogloss:0.327706\teval-mlogloss:0.349715\n",
      "[32]\ttrain-mlogloss:0.322669\teval-mlogloss:0.345448\n",
      "[33]\ttrain-mlogloss:0.317996\teval-mlogloss:0.341379\n",
      "[34]\ttrain-mlogloss:0.313423\teval-mlogloss:0.337396\n",
      "[35]\ttrain-mlogloss:0.309033\teval-mlogloss:0.333743\n",
      "[36]\ttrain-mlogloss:0.3045\teval-mlogloss:0.32996\n",
      "[37]\ttrain-mlogloss:0.300281\teval-mlogloss:0.326444\n",
      "[38]\ttrain-mlogloss:0.296355\teval-mlogloss:0.323113\n",
      "[39]\ttrain-mlogloss:0.292359\teval-mlogloss:0.319787\n",
      "[40]\ttrain-mlogloss:0.288581\teval-mlogloss:0.316698\n",
      "[41]\ttrain-mlogloss:0.284967\teval-mlogloss:0.313693\n",
      "[42]\ttrain-mlogloss:0.281416\teval-mlogloss:0.31087\n",
      "[43]\ttrain-mlogloss:0.278214\teval-mlogloss:0.308167\n",
      "[44]\ttrain-mlogloss:0.275054\teval-mlogloss:0.305731\n",
      "[45]\ttrain-mlogloss:0.272024\teval-mlogloss:0.303207\n",
      "[46]\ttrain-mlogloss:0.269225\teval-mlogloss:0.301057\n",
      "[47]\ttrain-mlogloss:0.266324\teval-mlogloss:0.298882\n",
      "[48]\ttrain-mlogloss:0.263606\teval-mlogloss:0.296634\n",
      "[49]\ttrain-mlogloss:0.260989\teval-mlogloss:0.294656\n",
      "[50]\ttrain-mlogloss:0.258466\teval-mlogloss:0.29277\n",
      "[51]\ttrain-mlogloss:0.255807\teval-mlogloss:0.29086\n",
      "[52]\ttrain-mlogloss:0.253303\teval-mlogloss:0.289063\n",
      "[53]\ttrain-mlogloss:0.25105\teval-mlogloss:0.287576\n",
      "[54]\ttrain-mlogloss:0.248731\teval-mlogloss:0.286055\n",
      "[55]\ttrain-mlogloss:0.246394\teval-mlogloss:0.28435\n",
      "[56]\ttrain-mlogloss:0.244272\teval-mlogloss:0.282935\n",
      "[57]\ttrain-mlogloss:0.242325\teval-mlogloss:0.281704\n",
      "[58]\ttrain-mlogloss:0.240181\teval-mlogloss:0.280357\n",
      "[59]\ttrain-mlogloss:0.238179\teval-mlogloss:0.279218\n",
      "[60]\ttrain-mlogloss:0.236221\teval-mlogloss:0.278088\n",
      "[61]\ttrain-mlogloss:0.23436\teval-mlogloss:0.277032\n",
      "[62]\ttrain-mlogloss:0.232541\teval-mlogloss:0.27609\n",
      "[63]\ttrain-mlogloss:0.230889\teval-mlogloss:0.275042\n",
      "[64]\ttrain-mlogloss:0.229302\teval-mlogloss:0.27408\n",
      "[65]\ttrain-mlogloss:0.227641\teval-mlogloss:0.273031\n",
      "[66]\ttrain-mlogloss:0.225834\teval-mlogloss:0.271955\n",
      "[67]\ttrain-mlogloss:0.224205\teval-mlogloss:0.270987\n",
      "[68]\ttrain-mlogloss:0.222492\teval-mlogloss:0.270237\n",
      "[69]\ttrain-mlogloss:0.221016\teval-mlogloss:0.269538\n",
      "[70]\ttrain-mlogloss:0.219468\teval-mlogloss:0.268943\n",
      "[71]\ttrain-mlogloss:0.217995\teval-mlogloss:0.268448\n",
      "[72]\ttrain-mlogloss:0.216624\teval-mlogloss:0.267884\n",
      "[73]\ttrain-mlogloss:0.21539\teval-mlogloss:0.267184\n",
      "[74]\ttrain-mlogloss:0.214037\teval-mlogloss:0.266561\n",
      "[75]\ttrain-mlogloss:0.212779\teval-mlogloss:0.265987\n",
      "[76]\ttrain-mlogloss:0.211658\teval-mlogloss:0.265657\n",
      "[77]\ttrain-mlogloss:0.210336\teval-mlogloss:0.265117\n",
      "[78]\ttrain-mlogloss:0.209016\teval-mlogloss:0.264858\n",
      "[79]\ttrain-mlogloss:0.207867\teval-mlogloss:0.264713\n",
      "[80]\ttrain-mlogloss:0.206613\teval-mlogloss:0.264472\n",
      "[81]\ttrain-mlogloss:0.205316\teval-mlogloss:0.263871\n",
      "[82]\ttrain-mlogloss:0.204037\teval-mlogloss:0.263584\n",
      "[83]\ttrain-mlogloss:0.202815\teval-mlogloss:0.263169\n",
      "[84]\ttrain-mlogloss:0.201717\teval-mlogloss:0.262861\n",
      "[85]\ttrain-mlogloss:0.20068\teval-mlogloss:0.262464\n",
      "[86]\ttrain-mlogloss:0.199438\teval-mlogloss:0.262158\n",
      "[87]\ttrain-mlogloss:0.198166\teval-mlogloss:0.261869\n",
      "[88]\ttrain-mlogloss:0.197068\teval-mlogloss:0.261254\n",
      "[89]\ttrain-mlogloss:0.196123\teval-mlogloss:0.261084\n",
      "[90]\ttrain-mlogloss:0.194916\teval-mlogloss:0.261214\n",
      "[91]\ttrain-mlogloss:0.194106\teval-mlogloss:0.260999\n",
      "[92]\ttrain-mlogloss:0.192995\teval-mlogloss:0.260854\n",
      "[93]\ttrain-mlogloss:0.191946\teval-mlogloss:0.260749\n",
      "[94]\ttrain-mlogloss:0.191022\teval-mlogloss:0.260644\n",
      "[95]\ttrain-mlogloss:0.189881\teval-mlogloss:0.260797\n",
      "[96]\ttrain-mlogloss:0.18871\teval-mlogloss:0.260994\n",
      "[97]\ttrain-mlogloss:0.187866\teval-mlogloss:0.260997\n",
      "[98]\ttrain-mlogloss:0.186928\teval-mlogloss:0.260977\n",
      "[99]\ttrain-mlogloss:0.185959\teval-mlogloss:0.260937\n",
      "[100]\ttrain-mlogloss:0.185037\teval-mlogloss:0.260891\n",
      "[101]\ttrain-mlogloss:0.18417\teval-mlogloss:0.260902\n",
      "[102]\ttrain-mlogloss:0.183285\teval-mlogloss:0.260576\n",
      "[103]\ttrain-mlogloss:0.182338\teval-mlogloss:0.26034\n",
      "[104]\ttrain-mlogloss:0.181475\teval-mlogloss:0.26027\n",
      "[105]\ttrain-mlogloss:0.180555\teval-mlogloss:0.260181\n",
      "[106]\ttrain-mlogloss:0.179691\teval-mlogloss:0.260148\n",
      "[107]\ttrain-mlogloss:0.178809\teval-mlogloss:0.260337\n",
      "[108]\ttrain-mlogloss:0.177982\teval-mlogloss:0.260224\n",
      "[109]\ttrain-mlogloss:0.17708\teval-mlogloss:0.260217\n",
      "[110]\ttrain-mlogloss:0.176296\teval-mlogloss:0.260212\n",
      "[111]\ttrain-mlogloss:0.175296\teval-mlogloss:0.260333\n",
      "[112]\ttrain-mlogloss:0.174425\teval-mlogloss:0.260288\n",
      "[113]\ttrain-mlogloss:0.173674\teval-mlogloss:0.260147\n",
      "[114]\ttrain-mlogloss:0.172758\teval-mlogloss:0.259956\n",
      "[115]\ttrain-mlogloss:0.171969\teval-mlogloss:0.260422\n",
      "[116]\ttrain-mlogloss:0.171177\teval-mlogloss:0.260725\n",
      "[117]\ttrain-mlogloss:0.170353\teval-mlogloss:0.26084\n",
      "[118]\ttrain-mlogloss:0.169462\teval-mlogloss:0.260951\n",
      "[119]\ttrain-mlogloss:0.168748\teval-mlogloss:0.260916\n",
      "[120]\ttrain-mlogloss:0.168108\teval-mlogloss:0.261127\n",
      "[121]\ttrain-mlogloss:0.167378\teval-mlogloss:0.26142\n",
      "[122]\ttrain-mlogloss:0.166675\teval-mlogloss:0.261577\n",
      "[123]\ttrain-mlogloss:0.165943\teval-mlogloss:0.262015\n",
      "[124]\ttrain-mlogloss:0.165252\teval-mlogloss:0.261982\n",
      "[125]\ttrain-mlogloss:0.164474\teval-mlogloss:0.262044\n",
      "[126]\ttrain-mlogloss:0.163731\teval-mlogloss:0.262411\n",
      "[127]\ttrain-mlogloss:0.162948\teval-mlogloss:0.262578\n",
      "[128]\ttrain-mlogloss:0.162255\teval-mlogloss:0.262518\n",
      "[129]\ttrain-mlogloss:0.161568\teval-mlogloss:0.262485\n",
      "[130]\ttrain-mlogloss:0.160764\teval-mlogloss:0.262656\n",
      "[131]\ttrain-mlogloss:0.160018\teval-mlogloss:0.262401\n",
      "[132]\ttrain-mlogloss:0.159475\teval-mlogloss:0.262447\n",
      "[133]\ttrain-mlogloss:0.158833\teval-mlogloss:0.262708\n",
      "[134]\ttrain-mlogloss:0.158166\teval-mlogloss:0.262844\n",
      "[135]\ttrain-mlogloss:0.157537\teval-mlogloss:0.26307\n",
      "[136]\ttrain-mlogloss:0.156833\teval-mlogloss:0.263203\n",
      "[137]\ttrain-mlogloss:0.156139\teval-mlogloss:0.263243\n",
      "[138]\ttrain-mlogloss:0.155432\teval-mlogloss:0.263565\n",
      "[139]\ttrain-mlogloss:0.154795\teval-mlogloss:0.263532\n",
      "[140]\ttrain-mlogloss:0.154039\teval-mlogloss:0.263946\n",
      "[141]\ttrain-mlogloss:0.153401\teval-mlogloss:0.263868\n",
      "[142]\ttrain-mlogloss:0.152687\teval-mlogloss:0.263797\n",
      "[143]\ttrain-mlogloss:0.152204\teval-mlogloss:0.263692\n",
      "[144]\ttrain-mlogloss:0.151513\teval-mlogloss:0.26389\n",
      "[145]\ttrain-mlogloss:0.150923\teval-mlogloss:0.264044\n",
      "[146]\ttrain-mlogloss:0.150342\teval-mlogloss:0.264021\n",
      "[147]\ttrain-mlogloss:0.149639\teval-mlogloss:0.263977\n",
      "[148]\ttrain-mlogloss:0.148968\teval-mlogloss:0.264055\n",
      "[149]\ttrain-mlogloss:0.148308\teval-mlogloss:0.264339\n",
      "[150]\ttrain-mlogloss:0.147791\teval-mlogloss:0.264519\n",
      "[151]\ttrain-mlogloss:0.147305\teval-mlogloss:0.264874\n",
      "[152]\ttrain-mlogloss:0.146781\teval-mlogloss:0.265004\n",
      "[153]\ttrain-mlogloss:0.14618\teval-mlogloss:0.265133\n",
      "[154]\ttrain-mlogloss:0.145578\teval-mlogloss:0.265222\n",
      "[155]\ttrain-mlogloss:0.145098\teval-mlogloss:0.265336\n",
      "[156]\ttrain-mlogloss:0.144621\teval-mlogloss:0.265572\n",
      "[157]\ttrain-mlogloss:0.144074\teval-mlogloss:0.265629\n",
      "[158]\ttrain-mlogloss:0.143531\teval-mlogloss:0.265684\n",
      "[159]\ttrain-mlogloss:0.14314\teval-mlogloss:0.265866\n",
      "[160]\ttrain-mlogloss:0.142648\teval-mlogloss:0.266025\n",
      "[161]\ttrain-mlogloss:0.142122\teval-mlogloss:0.266372\n",
      "[162]\ttrain-mlogloss:0.141634\teval-mlogloss:0.266443\n",
      "[163]\ttrain-mlogloss:0.14116\teval-mlogloss:0.266593\n",
      "[164]\ttrain-mlogloss:0.140747\teval-mlogloss:0.266791\n",
      "[165]\ttrain-mlogloss:0.140261\teval-mlogloss:0.267054\n",
      "[166]\ttrain-mlogloss:0.139805\teval-mlogloss:0.267442\n",
      "[167]\ttrain-mlogloss:0.139333\teval-mlogloss:0.267723\n",
      "[168]\ttrain-mlogloss:0.138892\teval-mlogloss:0.268083\n",
      "[169]\ttrain-mlogloss:0.138439\teval-mlogloss:0.268486\n",
      "[170]\ttrain-mlogloss:0.137936\teval-mlogloss:0.268587\n",
      "[171]\ttrain-mlogloss:0.137474\teval-mlogloss:0.268775\n",
      "[172]\ttrain-mlogloss:0.137083\teval-mlogloss:0.269037\n",
      "[173]\ttrain-mlogloss:0.136745\teval-mlogloss:0.269178\n",
      "[174]\ttrain-mlogloss:0.13628\teval-mlogloss:0.269483\n",
      "[175]\ttrain-mlogloss:0.135886\teval-mlogloss:0.269532\n",
      "[176]\ttrain-mlogloss:0.135419\teval-mlogloss:0.26964\n",
      "[177]\ttrain-mlogloss:0.134933\teval-mlogloss:0.269721\n",
      "[178]\ttrain-mlogloss:0.13447\teval-mlogloss:0.269752\n",
      "[179]\ttrain-mlogloss:0.134044\teval-mlogloss:0.269852\n",
      "[180]\ttrain-mlogloss:0.133568\teval-mlogloss:0.269807\n",
      "[181]\ttrain-mlogloss:0.133034\teval-mlogloss:0.270126\n",
      "[182]\ttrain-mlogloss:0.132582\teval-mlogloss:0.270342\n",
      "[183]\ttrain-mlogloss:0.132075\teval-mlogloss:0.27059\n",
      "[184]\ttrain-mlogloss:0.131693\teval-mlogloss:0.270662\n",
      "[185]\ttrain-mlogloss:0.13132\teval-mlogloss:0.270732\n",
      "[186]\ttrain-mlogloss:0.13089\teval-mlogloss:0.270636\n",
      "[187]\ttrain-mlogloss:0.130587\teval-mlogloss:0.270664\n",
      "[188]\ttrain-mlogloss:0.130244\teval-mlogloss:0.270771\n",
      "[189]\ttrain-mlogloss:0.129837\teval-mlogloss:0.271118\n",
      "[190]\ttrain-mlogloss:0.12941\teval-mlogloss:0.271219\n",
      "[191]\ttrain-mlogloss:0.128944\teval-mlogloss:0.27103\n",
      "[192]\ttrain-mlogloss:0.128583\teval-mlogloss:0.271092\n",
      "[193]\ttrain-mlogloss:0.128272\teval-mlogloss:0.271118\n",
      "[194]\ttrain-mlogloss:0.127908\teval-mlogloss:0.271169\n",
      "[195]\ttrain-mlogloss:0.127481\teval-mlogloss:0.271031\n",
      "[196]\ttrain-mlogloss:0.127152\teval-mlogloss:0.271186\n",
      "[197]\ttrain-mlogloss:0.126748\teval-mlogloss:0.271261\n",
      "[198]\ttrain-mlogloss:0.126284\teval-mlogloss:0.271077\n",
      "[199]\ttrain-mlogloss:0.125882\teval-mlogloss:0.271252\n",
      "[200]\ttrain-mlogloss:0.125538\teval-mlogloss:0.271253\n",
      "[201]\ttrain-mlogloss:0.125219\teval-mlogloss:0.271247\n",
      "[202]\ttrain-mlogloss:0.124909\teval-mlogloss:0.271274\n",
      "[203]\ttrain-mlogloss:0.124469\teval-mlogloss:0.271359\n",
      "[204]\ttrain-mlogloss:0.124117\teval-mlogloss:0.271471\n",
      "[205]\ttrain-mlogloss:0.123759\teval-mlogloss:0.271653\n",
      "[206]\ttrain-mlogloss:0.123474\teval-mlogloss:0.272049\n",
      "[207]\ttrain-mlogloss:0.12318\teval-mlogloss:0.272199\n",
      "[208]\ttrain-mlogloss:0.122784\teval-mlogloss:0.272268\n",
      "[209]\ttrain-mlogloss:0.12239\teval-mlogloss:0.272353\n",
      "[210]\ttrain-mlogloss:0.12216\teval-mlogloss:0.272464\n",
      "[211]\ttrain-mlogloss:0.121773\teval-mlogloss:0.272821\n",
      "[212]\ttrain-mlogloss:0.121399\teval-mlogloss:0.2732\n",
      "[213]\ttrain-mlogloss:0.121007\teval-mlogloss:0.273279\n",
      "[214]\ttrain-mlogloss:0.120716\teval-mlogloss:0.27317\n",
      "Stopping. Best iteration:\n",
      "[114]\ttrain-mlogloss:0.172758\teval-mlogloss:0.259956\n",
      "\n",
      "xgb now score is: [2.4208301225770263, 2.2433633135072886, 2.5190920314658434]\n",
      "[0]\ttrain-mlogloss:0.671351\teval-mlogloss:0.670851\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[1]\ttrain-mlogloss:0.65071\teval-mlogloss:0.649924\n",
      "[2]\ttrain-mlogloss:0.631029\teval-mlogloss:0.629969\n",
      "[3]\ttrain-mlogloss:0.612501\teval-mlogloss:0.611022\n",
      "[4]\ttrain-mlogloss:0.594878\teval-mlogloss:0.59284\n",
      "[5]\ttrain-mlogloss:0.578212\teval-mlogloss:0.57571\n",
      "[6]\ttrain-mlogloss:0.562261\teval-mlogloss:0.559446\n",
      "[7]\ttrain-mlogloss:0.547332\teval-mlogloss:0.544003\n",
      "[8]\ttrain-mlogloss:0.533241\teval-mlogloss:0.529621\n",
      "[9]\ttrain-mlogloss:0.519595\teval-mlogloss:0.515554\n",
      "[10]\ttrain-mlogloss:0.506753\teval-mlogloss:0.502374\n",
      "[11]\ttrain-mlogloss:0.494611\teval-mlogloss:0.489661\n",
      "[12]\ttrain-mlogloss:0.482942\teval-mlogloss:0.477767\n",
      "[13]\ttrain-mlogloss:0.471973\teval-mlogloss:0.46646\n",
      "[14]\ttrain-mlogloss:0.461304\teval-mlogloss:0.455362\n",
      "[15]\ttrain-mlogloss:0.451214\teval-mlogloss:0.444875\n",
      "[16]\ttrain-mlogloss:0.441522\teval-mlogloss:0.434936\n",
      "[17]\ttrain-mlogloss:0.432417\teval-mlogloss:0.425672\n",
      "[18]\ttrain-mlogloss:0.423548\teval-mlogloss:0.416794\n",
      "[19]\ttrain-mlogloss:0.4151\teval-mlogloss:0.408211\n",
      "[20]\ttrain-mlogloss:0.406956\teval-mlogloss:0.399919\n",
      "[21]\ttrain-mlogloss:0.399204\teval-mlogloss:0.391909\n",
      "[22]\ttrain-mlogloss:0.39163\teval-mlogloss:0.384176\n",
      "[23]\ttrain-mlogloss:0.384707\teval-mlogloss:0.377111\n",
      "[24]\ttrain-mlogloss:0.377829\teval-mlogloss:0.369974\n",
      "[25]\ttrain-mlogloss:0.371367\teval-mlogloss:0.363463\n",
      "[26]\ttrain-mlogloss:0.365198\teval-mlogloss:0.357096\n",
      "[27]\ttrain-mlogloss:0.359051\teval-mlogloss:0.350934\n",
      "[28]\ttrain-mlogloss:0.353386\teval-mlogloss:0.344918\n",
      "[29]\ttrain-mlogloss:0.347889\teval-mlogloss:0.339319\n",
      "[30]\ttrain-mlogloss:0.342663\teval-mlogloss:0.334047\n",
      "[31]\ttrain-mlogloss:0.337586\teval-mlogloss:0.328826\n",
      "[32]\ttrain-mlogloss:0.332822\teval-mlogloss:0.323989\n",
      "[33]\ttrain-mlogloss:0.328259\teval-mlogloss:0.31941\n",
      "[34]\ttrain-mlogloss:0.323845\teval-mlogloss:0.314952\n",
      "[35]\ttrain-mlogloss:0.319628\teval-mlogloss:0.310766\n",
      "[36]\ttrain-mlogloss:0.315661\teval-mlogloss:0.3067\n",
      "[37]\ttrain-mlogloss:0.311687\teval-mlogloss:0.302617\n",
      "[38]\ttrain-mlogloss:0.307891\teval-mlogloss:0.298862\n",
      "[39]\ttrain-mlogloss:0.30421\teval-mlogloss:0.295156\n",
      "[40]\ttrain-mlogloss:0.300587\teval-mlogloss:0.291502\n",
      "[41]\ttrain-mlogloss:0.297259\teval-mlogloss:0.288205\n",
      "[42]\ttrain-mlogloss:0.293976\teval-mlogloss:0.284784\n",
      "[43]\ttrain-mlogloss:0.290812\teval-mlogloss:0.281659\n",
      "[44]\ttrain-mlogloss:0.287841\teval-mlogloss:0.278745\n",
      "[45]\ttrain-mlogloss:0.284792\teval-mlogloss:0.275869\n",
      "[46]\ttrain-mlogloss:0.281843\teval-mlogloss:0.272945\n",
      "[47]\ttrain-mlogloss:0.279104\teval-mlogloss:0.270288\n",
      "[48]\ttrain-mlogloss:0.276387\teval-mlogloss:0.267662\n",
      "[49]\ttrain-mlogloss:0.273778\teval-mlogloss:0.265382\n",
      "[50]\ttrain-mlogloss:0.271241\teval-mlogloss:0.263015\n",
      "[51]\ttrain-mlogloss:0.268805\teval-mlogloss:0.260931\n",
      "[52]\ttrain-mlogloss:0.266409\teval-mlogloss:0.258683\n",
      "[53]\ttrain-mlogloss:0.264189\teval-mlogloss:0.256644\n",
      "[54]\ttrain-mlogloss:0.261959\teval-mlogloss:0.254614\n",
      "[55]\ttrain-mlogloss:0.259826\teval-mlogloss:0.252719\n",
      "[56]\ttrain-mlogloss:0.257556\teval-mlogloss:0.250977\n",
      "[57]\ttrain-mlogloss:0.255408\teval-mlogloss:0.249409\n",
      "[58]\ttrain-mlogloss:0.253241\teval-mlogloss:0.247623\n",
      "[59]\ttrain-mlogloss:0.251328\teval-mlogloss:0.246104\n",
      "[60]\ttrain-mlogloss:0.249299\teval-mlogloss:0.244677\n",
      "[61]\ttrain-mlogloss:0.247455\teval-mlogloss:0.242967\n",
      "[62]\ttrain-mlogloss:0.245615\teval-mlogloss:0.241562\n",
      "[63]\ttrain-mlogloss:0.243636\teval-mlogloss:0.240234\n",
      "[64]\ttrain-mlogloss:0.241999\teval-mlogloss:0.238845\n",
      "[65]\ttrain-mlogloss:0.240345\teval-mlogloss:0.237429\n",
      "[66]\ttrain-mlogloss:0.238823\teval-mlogloss:0.236113\n",
      "[67]\ttrain-mlogloss:0.237227\teval-mlogloss:0.234737\n",
      "[68]\ttrain-mlogloss:0.235461\teval-mlogloss:0.233627\n",
      "[69]\ttrain-mlogloss:0.233793\teval-mlogloss:0.232489\n",
      "[70]\ttrain-mlogloss:0.232165\teval-mlogloss:0.23146\n",
      "[71]\ttrain-mlogloss:0.230648\teval-mlogloss:0.230242\n",
      "[72]\ttrain-mlogloss:0.229172\teval-mlogloss:0.229272\n",
      "[73]\ttrain-mlogloss:0.227592\teval-mlogloss:0.228629\n",
      "[74]\ttrain-mlogloss:0.226223\teval-mlogloss:0.227793\n",
      "[75]\ttrain-mlogloss:0.224855\teval-mlogloss:0.226954\n",
      "[76]\ttrain-mlogloss:0.223444\teval-mlogloss:0.225991\n",
      "[77]\ttrain-mlogloss:0.222064\teval-mlogloss:0.225409\n",
      "[78]\ttrain-mlogloss:0.220512\teval-mlogloss:0.22466\n",
      "[79]\ttrain-mlogloss:0.219316\teval-mlogloss:0.223802\n",
      "[80]\ttrain-mlogloss:0.217954\teval-mlogloss:0.223126\n",
      "[81]\ttrain-mlogloss:0.21669\teval-mlogloss:0.222735\n",
      "[82]\ttrain-mlogloss:0.21507\teval-mlogloss:0.222073\n",
      "[83]\ttrain-mlogloss:0.213811\teval-mlogloss:0.221256\n",
      "[84]\ttrain-mlogloss:0.212599\teval-mlogloss:0.220784\n",
      "[85]\ttrain-mlogloss:0.211248\teval-mlogloss:0.220266\n",
      "[86]\ttrain-mlogloss:0.210124\teval-mlogloss:0.220095\n",
      "[87]\ttrain-mlogloss:0.208777\teval-mlogloss:0.219707\n",
      "[88]\ttrain-mlogloss:0.207488\teval-mlogloss:0.219162\n",
      "[89]\ttrain-mlogloss:0.206258\teval-mlogloss:0.218838\n",
      "[90]\ttrain-mlogloss:0.205318\teval-mlogloss:0.218421\n",
      "[91]\ttrain-mlogloss:0.204052\teval-mlogloss:0.217987\n",
      "[92]\ttrain-mlogloss:0.202882\teval-mlogloss:0.217699\n",
      "[93]\ttrain-mlogloss:0.201733\teval-mlogloss:0.217462\n",
      "[94]\ttrain-mlogloss:0.200474\teval-mlogloss:0.217073\n",
      "[95]\ttrain-mlogloss:0.199323\teval-mlogloss:0.217037\n",
      "[96]\ttrain-mlogloss:0.198283\teval-mlogloss:0.216905\n",
      "[97]\ttrain-mlogloss:0.19722\teval-mlogloss:0.216757\n",
      "[98]\ttrain-mlogloss:0.196397\teval-mlogloss:0.216624\n",
      "[99]\ttrain-mlogloss:0.195472\teval-mlogloss:0.216133\n",
      "[100]\ttrain-mlogloss:0.194624\teval-mlogloss:0.215985\n",
      "[101]\ttrain-mlogloss:0.193623\teval-mlogloss:0.215821\n",
      "[102]\ttrain-mlogloss:0.1928\teval-mlogloss:0.215626\n",
      "[103]\ttrain-mlogloss:0.1918\teval-mlogloss:0.21542\n",
      "[104]\ttrain-mlogloss:0.191074\teval-mlogloss:0.215286\n",
      "[105]\ttrain-mlogloss:0.190184\teval-mlogloss:0.215004\n",
      "[106]\ttrain-mlogloss:0.189138\teval-mlogloss:0.214811\n",
      "[107]\ttrain-mlogloss:0.188158\teval-mlogloss:0.214653\n",
      "[108]\ttrain-mlogloss:0.187271\teval-mlogloss:0.214219\n",
      "[109]\ttrain-mlogloss:0.186408\teval-mlogloss:0.213748\n",
      "[110]\ttrain-mlogloss:0.185507\teval-mlogloss:0.21381\n",
      "[111]\ttrain-mlogloss:0.184665\teval-mlogloss:0.213521\n",
      "[112]\ttrain-mlogloss:0.183913\teval-mlogloss:0.213508\n",
      "[113]\ttrain-mlogloss:0.183074\teval-mlogloss:0.213349\n",
      "[114]\ttrain-mlogloss:0.182264\teval-mlogloss:0.21334\n",
      "[115]\ttrain-mlogloss:0.181398\teval-mlogloss:0.213238\n",
      "[116]\ttrain-mlogloss:0.180456\teval-mlogloss:0.212927\n",
      "[117]\ttrain-mlogloss:0.179606\teval-mlogloss:0.212854\n",
      "[118]\ttrain-mlogloss:0.178855\teval-mlogloss:0.21271\n",
      "[119]\ttrain-mlogloss:0.178004\teval-mlogloss:0.212716\n",
      "[120]\ttrain-mlogloss:0.177283\teval-mlogloss:0.212888\n",
      "[121]\ttrain-mlogloss:0.176451\teval-mlogloss:0.21294\n",
      "[122]\ttrain-mlogloss:0.17556\teval-mlogloss:0.212939\n",
      "[123]\ttrain-mlogloss:0.174693\teval-mlogloss:0.212908\n",
      "[124]\ttrain-mlogloss:0.173814\teval-mlogloss:0.212936\n",
      "[125]\ttrain-mlogloss:0.172969\teval-mlogloss:0.212996\n",
      "[126]\ttrain-mlogloss:0.172184\teval-mlogloss:0.213023\n",
      "[127]\ttrain-mlogloss:0.171456\teval-mlogloss:0.21296\n",
      "[128]\ttrain-mlogloss:0.170792\teval-mlogloss:0.212726\n",
      "[129]\ttrain-mlogloss:0.170137\teval-mlogloss:0.212939\n",
      "[130]\ttrain-mlogloss:0.169451\teval-mlogloss:0.212942\n",
      "[131]\ttrain-mlogloss:0.168966\teval-mlogloss:0.213101\n",
      "[132]\ttrain-mlogloss:0.168071\teval-mlogloss:0.21326\n",
      "[133]\ttrain-mlogloss:0.167269\teval-mlogloss:0.213236\n",
      "[134]\ttrain-mlogloss:0.166646\teval-mlogloss:0.213377\n",
      "[135]\ttrain-mlogloss:0.166057\teval-mlogloss:0.213613\n",
      "[136]\ttrain-mlogloss:0.165236\teval-mlogloss:0.213892\n",
      "[137]\ttrain-mlogloss:0.164569\teval-mlogloss:0.213862\n",
      "[138]\ttrain-mlogloss:0.163953\teval-mlogloss:0.213784\n",
      "[139]\ttrain-mlogloss:0.163282\teval-mlogloss:0.213603\n",
      "[140]\ttrain-mlogloss:0.162574\teval-mlogloss:0.21366\n",
      "[141]\ttrain-mlogloss:0.161867\teval-mlogloss:0.213744\n",
      "[142]\ttrain-mlogloss:0.16129\teval-mlogloss:0.214048\n",
      "[143]\ttrain-mlogloss:0.160591\teval-mlogloss:0.213831\n",
      "[144]\ttrain-mlogloss:0.160132\teval-mlogloss:0.213862\n",
      "[145]\ttrain-mlogloss:0.159486\teval-mlogloss:0.21375\n",
      "[146]\ttrain-mlogloss:0.158923\teval-mlogloss:0.21381\n",
      "[147]\ttrain-mlogloss:0.15838\teval-mlogloss:0.213758\n",
      "[148]\ttrain-mlogloss:0.157697\teval-mlogloss:0.214095\n",
      "[149]\ttrain-mlogloss:0.157062\teval-mlogloss:0.21405\n",
      "[150]\ttrain-mlogloss:0.156403\teval-mlogloss:0.21422\n",
      "[151]\ttrain-mlogloss:0.155727\teval-mlogloss:0.214105\n",
      "[152]\ttrain-mlogloss:0.155131\teval-mlogloss:0.214328\n",
      "[153]\ttrain-mlogloss:0.154681\teval-mlogloss:0.214381\n",
      "[154]\ttrain-mlogloss:0.154122\teval-mlogloss:0.214441\n",
      "[155]\ttrain-mlogloss:0.153405\teval-mlogloss:0.214592\n",
      "[156]\ttrain-mlogloss:0.152837\teval-mlogloss:0.214785\n",
      "[157]\ttrain-mlogloss:0.152223\teval-mlogloss:0.214882\n",
      "[158]\ttrain-mlogloss:0.151758\teval-mlogloss:0.21502\n",
      "[159]\ttrain-mlogloss:0.151277\teval-mlogloss:0.215139\n",
      "[160]\ttrain-mlogloss:0.150612\teval-mlogloss:0.215361\n",
      "[161]\ttrain-mlogloss:0.15008\teval-mlogloss:0.215311\n",
      "[162]\ttrain-mlogloss:0.149545\teval-mlogloss:0.21537\n",
      "[163]\ttrain-mlogloss:0.148913\teval-mlogloss:0.215094\n",
      "[164]\ttrain-mlogloss:0.148344\teval-mlogloss:0.215155\n",
      "[165]\ttrain-mlogloss:0.147809\teval-mlogloss:0.21516\n",
      "[166]\ttrain-mlogloss:0.147285\teval-mlogloss:0.215239\n",
      "[167]\ttrain-mlogloss:0.146722\teval-mlogloss:0.21525\n",
      "[168]\ttrain-mlogloss:0.146286\teval-mlogloss:0.215597\n",
      "[169]\ttrain-mlogloss:0.145718\teval-mlogloss:0.215715\n",
      "[170]\ttrain-mlogloss:0.145268\teval-mlogloss:0.21573\n",
      "[171]\ttrain-mlogloss:0.144676\teval-mlogloss:0.215731\n",
      "[172]\ttrain-mlogloss:0.144177\teval-mlogloss:0.21576\n",
      "[173]\ttrain-mlogloss:0.143676\teval-mlogloss:0.215925\n",
      "[174]\ttrain-mlogloss:0.143267\teval-mlogloss:0.215929\n",
      "[175]\ttrain-mlogloss:0.142807\teval-mlogloss:0.216147\n",
      "[176]\ttrain-mlogloss:0.142247\teval-mlogloss:0.216196\n",
      "[177]\ttrain-mlogloss:0.141851\teval-mlogloss:0.216213\n",
      "[178]\ttrain-mlogloss:0.141399\teval-mlogloss:0.216146\n",
      "[179]\ttrain-mlogloss:0.140999\teval-mlogloss:0.216163\n",
      "[180]\ttrain-mlogloss:0.140548\teval-mlogloss:0.2161\n",
      "[181]\ttrain-mlogloss:0.140044\teval-mlogloss:0.216374\n",
      "[182]\ttrain-mlogloss:0.139594\teval-mlogloss:0.216577\n",
      "[183]\ttrain-mlogloss:0.139091\teval-mlogloss:0.216724\n",
      "[184]\ttrain-mlogloss:0.138698\teval-mlogloss:0.216577\n",
      "[185]\ttrain-mlogloss:0.138298\teval-mlogloss:0.216677\n",
      "[186]\ttrain-mlogloss:0.137918\teval-mlogloss:0.216625\n",
      "[187]\ttrain-mlogloss:0.137485\teval-mlogloss:0.216724\n",
      "[188]\ttrain-mlogloss:0.137036\teval-mlogloss:0.216824\n",
      "[189]\ttrain-mlogloss:0.136589\teval-mlogloss:0.217106\n",
      "[190]\ttrain-mlogloss:0.136096\teval-mlogloss:0.217219\n",
      "[191]\ttrain-mlogloss:0.135725\teval-mlogloss:0.217387\n",
      "[192]\ttrain-mlogloss:0.135345\teval-mlogloss:0.217365\n",
      "[193]\ttrain-mlogloss:0.134968\teval-mlogloss:0.21751\n",
      "[194]\ttrain-mlogloss:0.134655\teval-mlogloss:0.217508\n",
      "[195]\ttrain-mlogloss:0.134277\teval-mlogloss:0.217492\n",
      "[196]\ttrain-mlogloss:0.133797\teval-mlogloss:0.217668\n",
      "[197]\ttrain-mlogloss:0.133378\teval-mlogloss:0.217822\n",
      "[198]\ttrain-mlogloss:0.13296\teval-mlogloss:0.217855\n",
      "[199]\ttrain-mlogloss:0.132548\teval-mlogloss:0.218006\n",
      "[200]\ttrain-mlogloss:0.132149\teval-mlogloss:0.218059\n",
      "[201]\ttrain-mlogloss:0.131688\teval-mlogloss:0.218291\n",
      "[202]\ttrain-mlogloss:0.131319\teval-mlogloss:0.21851\n",
      "[203]\ttrain-mlogloss:0.130854\teval-mlogloss:0.218818\n",
      "[204]\ttrain-mlogloss:0.130524\teval-mlogloss:0.218916\n",
      "[205]\ttrain-mlogloss:0.13017\teval-mlogloss:0.21905\n",
      "[206]\ttrain-mlogloss:0.129731\teval-mlogloss:0.219396\n",
      "[207]\ttrain-mlogloss:0.129348\teval-mlogloss:0.219671\n",
      "[208]\ttrain-mlogloss:0.129027\teval-mlogloss:0.219854\n",
      "[209]\ttrain-mlogloss:0.128675\teval-mlogloss:0.220113\n",
      "[210]\ttrain-mlogloss:0.128353\teval-mlogloss:0.220227\n",
      "[211]\ttrain-mlogloss:0.12798\teval-mlogloss:0.220307\n",
      "[212]\ttrain-mlogloss:0.127586\teval-mlogloss:0.220516\n",
      "[213]\ttrain-mlogloss:0.127209\teval-mlogloss:0.220704\n",
      "[214]\ttrain-mlogloss:0.126715\teval-mlogloss:0.220624\n",
      "[215]\ttrain-mlogloss:0.126386\teval-mlogloss:0.220559\n",
      "[216]\ttrain-mlogloss:0.126057\teval-mlogloss:0.220746\n",
      "[217]\ttrain-mlogloss:0.125555\teval-mlogloss:0.220936\n",
      "[218]\ttrain-mlogloss:0.12521\teval-mlogloss:0.221143\n",
      "Stopping. Best iteration:\n",
      "[118]\ttrain-mlogloss:0.178855\teval-mlogloss:0.21271\n",
      "\n",
      "xgb now score is: [2.4208301225770263, 2.2433633135072886, 2.5190920314658434, 2.4902898448798805]\n",
      "[0]\ttrain-mlogloss:0.671172\teval-mlogloss:0.670892\n",
      "Multiple eval metrics have been passed: 'eval-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mlogloss hasn't improved in 100 rounds.\n",
      "[1]\ttrain-mlogloss:0.650364\teval-mlogloss:0.649909\n",
      "[2]\ttrain-mlogloss:0.630776\teval-mlogloss:0.629961\n",
      "[3]\ttrain-mlogloss:0.612026\teval-mlogloss:0.611159\n",
      "[4]\ttrain-mlogloss:0.594581\teval-mlogloss:0.593619\n",
      "[5]\ttrain-mlogloss:0.577666\teval-mlogloss:0.576861\n",
      "[6]\ttrain-mlogloss:0.561839\teval-mlogloss:0.560768\n",
      "[7]\ttrain-mlogloss:0.546732\teval-mlogloss:0.545437\n",
      "[8]\ttrain-mlogloss:0.532432\teval-mlogloss:0.530875\n",
      "[9]\ttrain-mlogloss:0.518787\teval-mlogloss:0.517006\n",
      "[10]\ttrain-mlogloss:0.505893\teval-mlogloss:0.504056\n",
      "[11]\ttrain-mlogloss:0.49319\teval-mlogloss:0.491082\n",
      "[12]\ttrain-mlogloss:0.481345\teval-mlogloss:0.479284\n",
      "[13]\ttrain-mlogloss:0.470207\teval-mlogloss:0.467954\n",
      "[14]\ttrain-mlogloss:0.459659\teval-mlogloss:0.457234\n",
      "[15]\ttrain-mlogloss:0.449237\teval-mlogloss:0.446705\n",
      "[16]\ttrain-mlogloss:0.439689\teval-mlogloss:0.436917\n",
      "[17]\ttrain-mlogloss:0.430303\teval-mlogloss:0.427354\n",
      "[18]\ttrain-mlogloss:0.421295\teval-mlogloss:0.418189\n",
      "[19]\ttrain-mlogloss:0.412725\teval-mlogloss:0.409759\n",
      "[20]\ttrain-mlogloss:0.404633\teval-mlogloss:0.40177\n",
      "[21]\ttrain-mlogloss:0.396794\teval-mlogloss:0.393791\n",
      "[22]\ttrain-mlogloss:0.389454\teval-mlogloss:0.386255\n",
      "[23]\ttrain-mlogloss:0.382271\teval-mlogloss:0.378961\n",
      "[24]\ttrain-mlogloss:0.375381\teval-mlogloss:0.371919\n",
      "[25]\ttrain-mlogloss:0.368948\teval-mlogloss:0.365418\n",
      "[26]\ttrain-mlogloss:0.362555\teval-mlogloss:0.358979\n",
      "[27]\ttrain-mlogloss:0.356441\teval-mlogloss:0.35287\n",
      "[28]\ttrain-mlogloss:0.350627\teval-mlogloss:0.347151\n",
      "[29]\ttrain-mlogloss:0.345076\teval-mlogloss:0.341759\n",
      "[30]\ttrain-mlogloss:0.339639\teval-mlogloss:0.336511\n",
      "[31]\ttrain-mlogloss:0.334494\teval-mlogloss:0.331409\n",
      "[32]\ttrain-mlogloss:0.329609\teval-mlogloss:0.326683\n",
      "[33]\ttrain-mlogloss:0.32498\teval-mlogloss:0.322159\n",
      "[34]\ttrain-mlogloss:0.320513\teval-mlogloss:0.317784\n",
      "[35]\ttrain-mlogloss:0.31612\teval-mlogloss:0.313693\n",
      "[36]\ttrain-mlogloss:0.311883\teval-mlogloss:0.309662\n",
      "[37]\ttrain-mlogloss:0.307926\teval-mlogloss:0.305604\n",
      "[38]\ttrain-mlogloss:0.303914\teval-mlogloss:0.301813\n",
      "[39]\ttrain-mlogloss:0.300258\teval-mlogloss:0.298352\n",
      "[40]\ttrain-mlogloss:0.296724\teval-mlogloss:0.29517\n",
      "[41]\ttrain-mlogloss:0.293308\teval-mlogloss:0.291851\n",
      "[42]\ttrain-mlogloss:0.2901\teval-mlogloss:0.28887\n",
      "[43]\ttrain-mlogloss:0.287033\teval-mlogloss:0.285659\n",
      "[44]\ttrain-mlogloss:0.283908\teval-mlogloss:0.282924\n",
      "[45]\ttrain-mlogloss:0.280794\teval-mlogloss:0.280156\n",
      "[46]\ttrain-mlogloss:0.277973\teval-mlogloss:0.277516\n",
      "[47]\ttrain-mlogloss:0.275236\teval-mlogloss:0.275035\n",
      "[48]\ttrain-mlogloss:0.272639\teval-mlogloss:0.272678\n",
      "[49]\ttrain-mlogloss:0.270048\teval-mlogloss:0.270401\n",
      "[50]\ttrain-mlogloss:0.267593\teval-mlogloss:0.268055\n",
      "[51]\ttrain-mlogloss:0.265229\teval-mlogloss:0.265862\n",
      "[52]\ttrain-mlogloss:0.262929\teval-mlogloss:0.263762\n",
      "[53]\ttrain-mlogloss:0.260761\teval-mlogloss:0.261654\n",
      "[54]\ttrain-mlogloss:0.258554\teval-mlogloss:0.259757\n",
      "[55]\ttrain-mlogloss:0.256306\teval-mlogloss:0.25795\n",
      "[56]\ttrain-mlogloss:0.25435\teval-mlogloss:0.256298\n",
      "[57]\ttrain-mlogloss:0.25254\teval-mlogloss:0.254625\n",
      "[58]\ttrain-mlogloss:0.250355\teval-mlogloss:0.253197\n",
      "[59]\ttrain-mlogloss:0.248692\teval-mlogloss:0.251643\n",
      "[60]\ttrain-mlogloss:0.246779\teval-mlogloss:0.250321\n",
      "[61]\ttrain-mlogloss:0.244913\teval-mlogloss:0.248732\n",
      "[62]\ttrain-mlogloss:0.243191\teval-mlogloss:0.24754\n",
      "[63]\ttrain-mlogloss:0.241587\teval-mlogloss:0.246403\n",
      "[64]\ttrain-mlogloss:0.239802\teval-mlogloss:0.24528\n",
      "[65]\ttrain-mlogloss:0.238254\teval-mlogloss:0.243976\n",
      "[66]\ttrain-mlogloss:0.236665\teval-mlogloss:0.243064\n",
      "[67]\ttrain-mlogloss:0.235059\teval-mlogloss:0.241743\n",
      "[68]\ttrain-mlogloss:0.233348\teval-mlogloss:0.240633\n",
      "[69]\ttrain-mlogloss:0.231995\teval-mlogloss:0.239723\n",
      "[70]\ttrain-mlogloss:0.23069\teval-mlogloss:0.23866\n",
      "[71]\ttrain-mlogloss:0.229126\teval-mlogloss:0.237692\n",
      "[72]\ttrain-mlogloss:0.227701\teval-mlogloss:0.236895\n",
      "[73]\ttrain-mlogloss:0.226308\teval-mlogloss:0.235885\n",
      "[74]\ttrain-mlogloss:0.224898\teval-mlogloss:0.234841\n",
      "[75]\ttrain-mlogloss:0.223496\teval-mlogloss:0.233847\n",
      "[76]\ttrain-mlogloss:0.222011\teval-mlogloss:0.23292\n",
      "[77]\ttrain-mlogloss:0.220507\teval-mlogloss:0.232333\n",
      "[78]\ttrain-mlogloss:0.219196\teval-mlogloss:0.231502\n",
      "[79]\ttrain-mlogloss:0.217885\teval-mlogloss:0.230804\n",
      "[80]\ttrain-mlogloss:0.216772\teval-mlogloss:0.230292\n",
      "[81]\ttrain-mlogloss:0.215345\teval-mlogloss:0.229888\n",
      "[82]\ttrain-mlogloss:0.213998\teval-mlogloss:0.229069\n",
      "[83]\ttrain-mlogloss:0.212883\teval-mlogloss:0.228375\n",
      "[84]\ttrain-mlogloss:0.211523\teval-mlogloss:0.228106\n",
      "[85]\ttrain-mlogloss:0.210191\teval-mlogloss:0.227481\n",
      "[86]\ttrain-mlogloss:0.209009\teval-mlogloss:0.226926\n",
      "[87]\ttrain-mlogloss:0.207754\teval-mlogloss:0.226289\n",
      "[88]\ttrain-mlogloss:0.206854\teval-mlogloss:0.225766\n",
      "[89]\ttrain-mlogloss:0.205738\teval-mlogloss:0.225186\n",
      "[90]\ttrain-mlogloss:0.204629\teval-mlogloss:0.224642\n",
      "[91]\ttrain-mlogloss:0.203486\teval-mlogloss:0.224185\n",
      "[92]\ttrain-mlogloss:0.20239\teval-mlogloss:0.223662\n",
      "[93]\ttrain-mlogloss:0.201362\teval-mlogloss:0.223335\n",
      "[94]\ttrain-mlogloss:0.200242\teval-mlogloss:0.223206\n",
      "[95]\ttrain-mlogloss:0.199295\teval-mlogloss:0.222656\n",
      "[96]\ttrain-mlogloss:0.198227\teval-mlogloss:0.222212\n",
      "[97]\ttrain-mlogloss:0.197116\teval-mlogloss:0.22204\n",
      "[98]\ttrain-mlogloss:0.196239\teval-mlogloss:0.221834\n",
      "[99]\ttrain-mlogloss:0.19549\teval-mlogloss:0.221658\n",
      "[100]\ttrain-mlogloss:0.194565\teval-mlogloss:0.221528\n",
      "[101]\ttrain-mlogloss:0.193776\teval-mlogloss:0.221323\n",
      "[102]\ttrain-mlogloss:0.192695\teval-mlogloss:0.220912\n",
      "[103]\ttrain-mlogloss:0.191788\teval-mlogloss:0.220772\n",
      "[104]\ttrain-mlogloss:0.190799\teval-mlogloss:0.220765\n",
      "[105]\ttrain-mlogloss:0.1897\teval-mlogloss:0.220609\n",
      "[106]\ttrain-mlogloss:0.188717\teval-mlogloss:0.22052\n",
      "[107]\ttrain-mlogloss:0.187766\teval-mlogloss:0.22028\n",
      "[108]\ttrain-mlogloss:0.186747\teval-mlogloss:0.220012\n",
      "[109]\ttrain-mlogloss:0.185859\teval-mlogloss:0.219701\n",
      "[110]\ttrain-mlogloss:0.184934\teval-mlogloss:0.219386\n",
      "[111]\ttrain-mlogloss:0.183869\teval-mlogloss:0.219369\n",
      "[112]\ttrain-mlogloss:0.183054\teval-mlogloss:0.219071\n",
      "[113]\ttrain-mlogloss:0.182006\teval-mlogloss:0.21883\n",
      "[114]\ttrain-mlogloss:0.18122\teval-mlogloss:0.218701\n",
      "[115]\ttrain-mlogloss:0.180345\teval-mlogloss:0.218552\n",
      "[116]\ttrain-mlogloss:0.179566\teval-mlogloss:0.218545\n",
      "[117]\ttrain-mlogloss:0.178622\teval-mlogloss:0.218885\n",
      "[118]\ttrain-mlogloss:0.177763\teval-mlogloss:0.218585\n",
      "[119]\ttrain-mlogloss:0.17711\teval-mlogloss:0.218485\n",
      "[120]\ttrain-mlogloss:0.176323\teval-mlogloss:0.218306\n",
      "[121]\ttrain-mlogloss:0.175592\teval-mlogloss:0.218213\n",
      "[122]\ttrain-mlogloss:0.174943\teval-mlogloss:0.218515\n",
      "[123]\ttrain-mlogloss:0.174291\teval-mlogloss:0.218531\n",
      "[124]\ttrain-mlogloss:0.173513\teval-mlogloss:0.218126\n",
      "[125]\ttrain-mlogloss:0.172851\teval-mlogloss:0.218177\n",
      "[126]\ttrain-mlogloss:0.172219\teval-mlogloss:0.218029\n",
      "[127]\ttrain-mlogloss:0.171512\teval-mlogloss:0.218057\n",
      "[128]\ttrain-mlogloss:0.170826\teval-mlogloss:0.218157\n",
      "[129]\ttrain-mlogloss:0.170133\teval-mlogloss:0.218185\n",
      "[130]\ttrain-mlogloss:0.169386\teval-mlogloss:0.218192\n",
      "[131]\ttrain-mlogloss:0.168659\teval-mlogloss:0.218367\n",
      "[132]\ttrain-mlogloss:0.167855\teval-mlogloss:0.218339\n",
      "[133]\ttrain-mlogloss:0.167153\teval-mlogloss:0.218573\n",
      "[134]\ttrain-mlogloss:0.166486\teval-mlogloss:0.218467\n",
      "[135]\ttrain-mlogloss:0.165779\teval-mlogloss:0.218513\n",
      "[136]\ttrain-mlogloss:0.165227\teval-mlogloss:0.218671\n",
      "[137]\ttrain-mlogloss:0.164577\teval-mlogloss:0.218753\n",
      "[138]\ttrain-mlogloss:0.163783\teval-mlogloss:0.21904\n",
      "[139]\ttrain-mlogloss:0.163084\teval-mlogloss:0.219089\n",
      "[140]\ttrain-mlogloss:0.162486\teval-mlogloss:0.219107\n",
      "[141]\ttrain-mlogloss:0.161957\teval-mlogloss:0.219063\n",
      "[142]\ttrain-mlogloss:0.161299\teval-mlogloss:0.219119\n",
      "[143]\ttrain-mlogloss:0.160649\teval-mlogloss:0.21911\n",
      "[144]\ttrain-mlogloss:0.159916\teval-mlogloss:0.21926\n",
      "[145]\ttrain-mlogloss:0.15939\teval-mlogloss:0.219218\n",
      "[146]\ttrain-mlogloss:0.158808\teval-mlogloss:0.219224\n",
      "[147]\ttrain-mlogloss:0.158246\teval-mlogloss:0.219165\n",
      "[148]\ttrain-mlogloss:0.157636\teval-mlogloss:0.219147\n",
      "[149]\ttrain-mlogloss:0.156954\teval-mlogloss:0.219198\n",
      "[150]\ttrain-mlogloss:0.156302\teval-mlogloss:0.219405\n",
      "[151]\ttrain-mlogloss:0.155687\teval-mlogloss:0.219618\n",
      "[152]\ttrain-mlogloss:0.154981\teval-mlogloss:0.219478\n",
      "[153]\ttrain-mlogloss:0.154363\teval-mlogloss:0.219395\n",
      "[154]\ttrain-mlogloss:0.153827\teval-mlogloss:0.219458\n",
      "[155]\ttrain-mlogloss:0.153251\teval-mlogloss:0.2195\n",
      "[156]\ttrain-mlogloss:0.152539\teval-mlogloss:0.219576\n",
      "[157]\ttrain-mlogloss:0.151958\teval-mlogloss:0.219504\n",
      "[158]\ttrain-mlogloss:0.151351\teval-mlogloss:0.219524\n",
      "[159]\ttrain-mlogloss:0.150828\teval-mlogloss:0.219732\n",
      "[160]\ttrain-mlogloss:0.150288\teval-mlogloss:0.219766\n",
      "[161]\ttrain-mlogloss:0.149733\teval-mlogloss:0.219766\n",
      "[162]\ttrain-mlogloss:0.149294\teval-mlogloss:0.220003\n",
      "[163]\ttrain-mlogloss:0.14872\teval-mlogloss:0.220234\n",
      "[164]\ttrain-mlogloss:0.148201\teval-mlogloss:0.220382\n",
      "[165]\ttrain-mlogloss:0.147728\teval-mlogloss:0.220716\n",
      "[166]\ttrain-mlogloss:0.147178\teval-mlogloss:0.220879\n",
      "[167]\ttrain-mlogloss:0.146652\teval-mlogloss:0.221065\n",
      "[168]\ttrain-mlogloss:0.146152\teval-mlogloss:0.221215\n",
      "[169]\ttrain-mlogloss:0.145518\teval-mlogloss:0.221271\n",
      "[170]\ttrain-mlogloss:0.145068\teval-mlogloss:0.221654\n",
      "[171]\ttrain-mlogloss:0.144591\teval-mlogloss:0.221656\n",
      "[172]\ttrain-mlogloss:0.144236\teval-mlogloss:0.221767\n",
      "[173]\ttrain-mlogloss:0.143882\teval-mlogloss:0.221918\n",
      "[174]\ttrain-mlogloss:0.143405\teval-mlogloss:0.221948\n",
      "[175]\ttrain-mlogloss:0.142986\teval-mlogloss:0.221778\n",
      "[176]\ttrain-mlogloss:0.142555\teval-mlogloss:0.222121\n",
      "[177]\ttrain-mlogloss:0.142026\teval-mlogloss:0.222096\n",
      "[178]\ttrain-mlogloss:0.141562\teval-mlogloss:0.22221\n",
      "[179]\ttrain-mlogloss:0.141168\teval-mlogloss:0.222274\n",
      "[180]\ttrain-mlogloss:0.140666\teval-mlogloss:0.222539\n",
      "[181]\ttrain-mlogloss:0.140166\teval-mlogloss:0.222593\n",
      "[182]\ttrain-mlogloss:0.139709\teval-mlogloss:0.222571\n",
      "[183]\ttrain-mlogloss:0.139322\teval-mlogloss:0.222478\n",
      "[184]\ttrain-mlogloss:0.138844\teval-mlogloss:0.22238\n",
      "[185]\ttrain-mlogloss:0.138421\teval-mlogloss:0.222515\n",
      "[186]\ttrain-mlogloss:0.137945\teval-mlogloss:0.222705\n",
      "[187]\ttrain-mlogloss:0.137526\teval-mlogloss:0.222622\n",
      "[188]\ttrain-mlogloss:0.137067\teval-mlogloss:0.222904\n",
      "[189]\ttrain-mlogloss:0.136708\teval-mlogloss:0.223088\n",
      "[190]\ttrain-mlogloss:0.136248\teval-mlogloss:0.223016\n",
      "[191]\ttrain-mlogloss:0.135918\teval-mlogloss:0.223076\n",
      "[192]\ttrain-mlogloss:0.135523\teval-mlogloss:0.2233\n",
      "[193]\ttrain-mlogloss:0.135077\teval-mlogloss:0.223508\n",
      "[194]\ttrain-mlogloss:0.134725\teval-mlogloss:0.223465\n",
      "[195]\ttrain-mlogloss:0.134337\teval-mlogloss:0.223671\n",
      "[196]\ttrain-mlogloss:0.133892\teval-mlogloss:0.223769\n",
      "[197]\ttrain-mlogloss:0.13343\teval-mlogloss:0.223849\n",
      "[198]\ttrain-mlogloss:0.132897\teval-mlogloss:0.224092\n",
      "[199]\ttrain-mlogloss:0.132451\teval-mlogloss:0.224415\n",
      "[200]\ttrain-mlogloss:0.132181\teval-mlogloss:0.224528\n",
      "[201]\ttrain-mlogloss:0.131748\teval-mlogloss:0.224698\n",
      "[202]\ttrain-mlogloss:0.131354\teval-mlogloss:0.225026\n",
      "[203]\ttrain-mlogloss:0.130992\teval-mlogloss:0.22515\n",
      "[204]\ttrain-mlogloss:0.130653\teval-mlogloss:0.225345\n",
      "[205]\ttrain-mlogloss:0.130283\teval-mlogloss:0.225442\n",
      "[206]\ttrain-mlogloss:0.129928\teval-mlogloss:0.225614\n",
      "[207]\ttrain-mlogloss:0.129539\teval-mlogloss:0.225888\n",
      "[208]\ttrain-mlogloss:0.129245\teval-mlogloss:0.225851\n",
      "[209]\ttrain-mlogloss:0.128903\teval-mlogloss:0.225979\n",
      "[210]\ttrain-mlogloss:0.128531\teval-mlogloss:0.226226\n",
      "[211]\ttrain-mlogloss:0.128047\teval-mlogloss:0.226588\n",
      "[212]\ttrain-mlogloss:0.127699\teval-mlogloss:0.226661\n",
      "[213]\ttrain-mlogloss:0.127326\teval-mlogloss:0.226556\n",
      "[214]\ttrain-mlogloss:0.127092\teval-mlogloss:0.22673\n",
      "[215]\ttrain-mlogloss:0.126753\teval-mlogloss:0.226703\n",
      "[216]\ttrain-mlogloss:0.126413\teval-mlogloss:0.226873\n",
      "[217]\ttrain-mlogloss:0.126103\teval-mlogloss:0.226769\n",
      "[218]\ttrain-mlogloss:0.125826\teval-mlogloss:0.226613\n",
      "[219]\ttrain-mlogloss:0.125576\teval-mlogloss:0.226732\n",
      "[220]\ttrain-mlogloss:0.125238\teval-mlogloss:0.226668\n",
      "[221]\ttrain-mlogloss:0.124892\teval-mlogloss:0.226583\n",
      "[222]\ttrain-mlogloss:0.124512\teval-mlogloss:0.226594\n",
      "[223]\ttrain-mlogloss:0.124228\teval-mlogloss:0.226704\n",
      "[224]\ttrain-mlogloss:0.123836\teval-mlogloss:0.226795\n",
      "[225]\ttrain-mlogloss:0.123557\teval-mlogloss:0.226906\n",
      "[226]\ttrain-mlogloss:0.123211\teval-mlogloss:0.226966\n",
      "Stopping. Best iteration:\n",
      "[126]\ttrain-mlogloss:0.172219\teval-mlogloss:0.218029\n",
      "\n",
      "xgb now score is: [2.4208301225770263, 2.2433633135072886, 2.5190920314658434, 2.4902898448798805, 2.5797977298125625]\n",
      "xgb_score_list: [2.4208301225770263, 2.2433633135072886, 2.5190920314658434, 2.4902898448798805, 2.5797977298125625]\n",
      "xgb_score_mean: 2.4506746084485203\n"
     ]
    }
   ],
   "source": [
    "clf_list = clf_list\n",
    "column_list = []\n",
    "train_data_list=[]\n",
    "test_data_list=[]\n",
    "for clf in clf_list:\n",
    "    train_data,test_data,clf_name=clf(x_train, y_train, x_valid, kf, label_split=None)\n",
    "    train_data_list.append(train_data)\n",
    "    test_data_list.append(test_data)\n",
    "train_stacking = np.concatenate(train_data_list, axis=1)\n",
    "test_stacking = np.concatenate(test_data_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原始特征和stacking特征合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 合并所有特征\n",
    "train = pd.DataFrame(np.concatenate([x_train, train_stacking], axis=1))\n",
    "test = np.concatenate([x_valid, test_stacking], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征重命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all = pd.DataFrame(train)\n",
    "df_train_all.columns = features_columns + clf_list_col\n",
    "df_test_all = pd.DataFrame(test)\n",
    "df_test_all.columns = features_columns + clf_list_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取数据ID以及特征标签LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all['user_id'] = all_data_test[~all_data_test['label'].isna()]['user_id']\n",
    "df_test_all['user_id'] = all_data_test[all_data_test['label'].isna()]['user_id']\n",
    "df_train_all['label'] = all_data_test[~all_data_test['label'].isna()]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练数据和测试数据保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_tain_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-75b75af723ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_tain_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_all.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_test_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_all.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_tain_all' is not defined"
     ]
    }
   ],
   "source": [
    "df_train_all.to_csv('train_all.csv',header=True,index=False)\n",
    "df_test_all.to_csv('test_all.csv',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
